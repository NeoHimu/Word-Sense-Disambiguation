{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.WARN)\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('../Glove/word_embedding_glove', 'rb')\n",
    "word_embedding = pickle.load(f)\n",
    "f.close()\n",
    "word_embedding = word_embedding[: len(word_embedding)-1]\n",
    "\n",
    "f = open('../Glove/vocab_glove', 'rb')\n",
    "vocab = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "word2id = dict((w, i) for i,w in enumerate(vocab))\n",
    "id2word = dict((i, w) for i,w in enumerate(vocab))\n",
    "\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "\n",
    "f = open(\"train.pickle\", 'rb')\n",
    "full_data = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Description\n",
    "sense_word = 'hard'\n",
    "model_name = 'basic'\n",
    "model_dir = 'output/' + sense_word + '/' + model_name\n",
    "save_dir = os.path.join(model_dir, \"save/\")\n",
    "log_dir = os.path.join(model_dir, \"log\")\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "mode = 'train'\n",
    "num_senses = 3\n",
    "batch_size = 64\n",
    "vocab_size = len(vocab)\n",
    "unk_vocab_size = 1\n",
    "word_emb_size = len(word_embedding[0])\n",
    "max_sent_size = 200\n",
    "hidden_size = 100\n",
    "keep_prob = 0.5\n",
    "l2_lambda = 0.001\n",
    "init_lr = 0.001\n",
    "decay_steps = 5000\n",
    "decay_rate = 0.96\n",
    "clip_norm = 1\n",
    "clipping = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "x = tf.placeholder('int32', [batch_size, max_sent_size], name=\"x\")\n",
    "y = tf.placeholder('int32', [batch_size], name=\"y\")\n",
    "x_mask  = tf.placeholder('bool', [batch_size, max_sent_size], name='x_mask') \n",
    "is_train = tf.placeholder('bool', [], name='is_train')\n",
    "word_emb_mat = tf.placeholder('float', [None, word_emb_size], name='emb_mat')\n",
    "input_keep_prob = tf.cond(is_train,lambda:keep_prob, lambda:tf.constant(1.0))\n",
    "x_len = tf.reduce_sum(tf.cast(x_mask, 'int32'), 1)\n",
    "\n",
    "with tf.name_scope(\"word_embedding\"):\n",
    "    if mode == 'train':\n",
    "        unk_word_emb_mat = tf.get_variable(\"word_emb_mat\", dtype='float', shape=[unk_vocab_size, word_emb_size], initializer=tf.contrib.layers.xavier_initializer(uniform=True, seed=None, dtype=tf.float32))\n",
    "    else:\n",
    "        unk_word_emb_mat = tf.get_variable(\"word_emb_mat\", shape=[unk_vocab_size, word_emb_size], dtype='float')\n",
    "        \n",
    "    final_word_emb_mat = tf.concat([word_emb_mat, unk_word_emb_mat], 0)\n",
    "    Wx = tf.nn.embedding_lookup(final_word_emb_mat, x)  \n",
    "\n",
    "with tf.variable_scope(\"lstm\"):\n",
    "    cell_fw = tf.contrib.rnn.BasicLSTMCell(hidden_size,state_is_tuple=True)\n",
    "    cell_bw = tf.contrib.rnn.BasicLSTMCell(hidden_size,state_is_tuple=True)\n",
    "\n",
    "    d_cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, input_keep_prob=input_keep_prob)\n",
    "    d_cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, input_keep_prob=input_keep_prob)\n",
    "    \n",
    "    (fw_h, bw_h), _ = tf.nn.bidirectional_dynamic_rnn(d_cell_fw, d_cell_bw, Wx, sequence_length=x_len, dtype='float', scope='lstm')\n",
    "    h = tf.concat([fw_h, bw_h], 2)\n",
    "\n",
    "def attention(input_x, input_mask, W_att):\n",
    "    h_masked = tf.boolean_mask(input_x, input_mask)\n",
    "    h_tanh = tf.tanh(h_masked)\n",
    "    u = tf.matmul(h_tanh, W_att)\n",
    "    a = tf.nn.softmax(u)\n",
    "    c = tf.reduce_sum(tf.multiply(h_tanh, a), 0)  \n",
    "    return c\n",
    "\n",
    "with tf.variable_scope(\"attention\"):\n",
    "    W_att = tf.Variable(tf.truncated_normal([2*hidden_size, 1], -0.1, 0.1), name=\"W_att\")\n",
    "#     W_att = tf.get_variable(\"W_att\", shape=[2*hidden_size, 1], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    c = tf.expand_dims(attention(h[0], x_mask[0], W_att), 0)\n",
    "    for i in range(1, batch_size):\n",
    "        c = tf.concat([c, tf.expand_dims(attention(h[i], x_mask[i], W_att), 0)], 0)\n",
    "        \n",
    "with tf.variable_scope(\"softmax_layer\"):\n",
    "    W = tf.Variable(tf.truncated_normal([2*hidden_size, num_senses], -0.1, 0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.zeros([num_senses]), name=\"b\")\n",
    "    drop_c = tf.nn.dropout(c, input_keep_prob)\n",
    "    logits = tf.matmul(drop_c, W) + b\n",
    "    predictions = tf.argmax(logits, 1)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(init_lr, global_step, decay_steps, decay_rate, staircase=True)\n",
    "\n",
    "tv_all = tf.trainable_variables()\n",
    "tv_regu =[]\n",
    "for t in tv_all:\n",
    "    if t.name.find('b:')==-1:\n",
    "        tv_regu.append(t)\n",
    "        \n",
    "# l2 Loss\n",
    "l2_loss = l2_lambda * tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv_regu ])\n",
    "\n",
    "total_loss = loss + l2_loss\n",
    "\n",
    "# Optimizer for loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "# Gradients and Variables for Loss\n",
    "grads_vars = optimizer.compute_gradients(total_loss)\n",
    "\n",
    "# Clipping of Gradients\n",
    "clipped_grads = grads_vars\n",
    "if(clipping == True):\n",
    "    clipped_grads = [(tf.clip_by_norm(grad, clip_norm), var) for grad, var in clipped_grads]\n",
    "\n",
    "# Training Optimizer for Total Loss\n",
    "train_op = optimizer.apply_gradients(clipped_grads, global_step=global_step)\n",
    "\n",
    "# Summaries\n",
    "var_summaries = []\n",
    "for v in tv_all:\n",
    "    var_summary = tf.summary.histogram(\"{}/var\".format(v.name), v)\n",
    "    var_summaries.append(var_summary)\n",
    "\n",
    "var_summaries_merged = tf.summary.merge(var_summaries)\n",
    "\n",
    "loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "total_loss_summary = tf.summary.scalar(\"total_loss\", total_loss)\n",
    "summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7597343218486361920\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7958282240\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 3290425736900701779\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:06:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "print (device_lib.list_local_devices())\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "sess.run(tf.global_variables_initializer())                          # For initializing all the variables\n",
    "saver = tf.train.Saver()                                             # For Saving the model\n",
    "summary_writer = tf.summary.FileWriter(log_dir, sess.graph)          # For writing Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    2    3 ..., 4330 4331 4332] TEST: [   1   17   22   23   30   31   33   34   36   39   42   45   49   57   61\n",
      "   70   72   85   87   88  104  117  118  125  130  134  138  142  148  156\n",
      "  159  162  170  173  182  185  189  192  195  196  202  203  214  217  218\n",
      "  220  221  223  224  226  227  248  252  253  286  289  291  295  298  302\n",
      "  304  308  311  322  326  333  345  347  352  378  380  381  383  398  402\n",
      "  405  408  410  415  418  429  452  453  457  463  465  467  470  472  481\n",
      "  482  483  485  491  496  499  501  502  514  517  520  521  526  527  528\n",
      "  530  531  536  538  543  545  546  553  555  556  559  562  566  567  569\n",
      "  575  576  578  581  582  587  598  599  607  615  620  621  629  634  636\n",
      "  638  639  641  648  651  661  663  666  668  678  683  684  685  690  692\n",
      "  704  706  708  712  716  720  735  748  768  773  775  776  788  794  799\n",
      "  803  809  817  825  829  836  841  842  843  861  866  867  871  872  882\n",
      "  883  888  891  895  896  898  900  910  917  918  925  927  933  935  938\n",
      "  944  949  953  962  965  971  977  983  985  991  996  998 1001 1002 1005\n",
      " 1009 1012 1021 1042 1043 1051 1062 1065 1068 1070 1073 1082 1084 1091 1098\n",
      " 1109 1114 1115 1116 1118 1119 1120 1129 1141 1144 1148 1160 1172 1176 1180\n",
      " 1185 1191 1203 1224 1230 1239 1242 1246 1248 1259 1261 1262 1267 1270 1271\n",
      " 1272 1273 1275 1276 1281 1282 1287 1288 1290 1303 1313 1319 1327 1328 1329\n",
      " 1332 1334 1343 1349 1351 1357 1358 1364 1366 1367 1368 1380 1385 1410 1419\n",
      " 1420 1423 1427 1430 1431 1432 1436 1445 1450 1452 1457 1473 1478 1482 1487\n",
      " 1490 1491 1495 1504 1509 1511 1521 1525 1529 1530 1536 1544 1554 1560 1563\n",
      " 1564 1571 1572 1580 1583 1593 1602 1615 1625 1632 1646 1647 1651 1652 1654\n",
      " 1656 1657 1658 1669 1675 1687 1691 1692 1696 1698 1699 1700 1702 1703 1708\n",
      " 1710 1712 1713 1715 1721 1726 1733 1738 1754 1760 1767 1775 1784 1789 1791\n",
      " 1798 1804 1824 1826 1828 1829 1830 1831 1832 1836 1875 1877 1878 1887 1888\n",
      " 1894 1895 1899 1907 1916 1917 1922 1931 1940 1945 1949 1950 1951 1953 1955\n",
      " 1965 1989 1990 1997 2013 2016 2017 2025 2027 2036 2041 2047 2052 2057 2058\n",
      " 2060 2070 2071 2072 2075 2076 2078 2081 2085 2087 2088 2089 2091 2095 2100\n",
      " 2102 2103 2106 2108 2110 2111 2115 2124 2125 2126 2134 2141 2161 2169 2171\n",
      " 2178 2188 2189 2192 2193 2194 2196 2198 2203 2210 2211 2212 2216 2224 2230\n",
      " 2232 2234 2250 2254 2255 2258 2263 2272 2275 2277 2280 2281 2285 2299 2301\n",
      " 2307 2308 2315 2318 2329 2334 2337 2339 2340 2345 2348 2351 2358 2365 2367\n",
      " 2369 2383 2396 2409 2411 2423 2425 2427 2434 2436 2438 2445 2450 2452 2453\n",
      " 2459 2465 2466 2470 2473 2480 2484 2485 2493 2500 2506 2507 2523 2525 2527\n",
      " 2528 2535 2543 2545 2551 2556 2558 2566 2567 2573 2577 2578 2590 2597 2601\n",
      " 2611 2615 2619 2620 2621 2622 2628 2631 2645 2653 2664 2672 2673 2678 2682\n",
      " 2686 2694 2696 2697 2701 2705 2706 2708 2722 2727 2729 2734 2737 2755 2760\n",
      " 2762 2764 2768 2771 2774 2788 2790 2796 2805 2816 2818 2823 2825 2829 2835\n",
      " 2839 2840 2844 2846 2849 2854 2859 2861 2868 2871 2889 2890 2905 2909 2911\n",
      " 2913 2914 2926 2931 2939 2942 2943 2946 2947 2955 2959 2961 2971 2975 2976\n",
      " 2980 2983 2984 2985 3001 3003 3005 3007 3008 3013 3015 3016 3021 3022 3025\n",
      " 3031 3049 3051 3069 3071 3073 3075 3076 3079 3080 3082 3083 3084 3094 3095\n",
      " 3097 3098 3099 3100 3104 3122 3140 3145 3155 3157 3159 3160 3173 3174 3177\n",
      " 3181 3184 3197 3198 3199 3201 3211 3212 3214 3217 3222 3232 3236 3240 3244\n",
      " 3250 3253 3261 3266 3293 3301 3303 3304 3307 3313 3325 3326 3329 3332 3339\n",
      " 3352 3363 3364 3367 3368 3369 3382 3384 3387 3389 3399 3405 3407 3412 3415\n",
      " 3422 3428 3434 3435 3453 3458 3469 3473 3474 3475 3481 3494 3495 3496 3497\n",
      " 3498 3500 3502 3503 3528 3532 3534 3538 3540 3556 3564 3565 3567 3571 3577\n",
      " 3581 3587 3592 3595 3600 3601 3605 3608 3614 3617 3618 3620 3624 3625 3626\n",
      " 3632 3640 3641 3648 3664 3667 3668 3670 3673 3690 3691 3696 3699 3700 3709\n",
      " 3714 3722 3725 3735 3736 3737 3739 3744 3759 3779 3786 3787 3788 3798 3805\n",
      " 3807 3809 3812 3818 3822 3823 3826 3831 3833 3838 3840 3841 3843 3848 3852\n",
      " 3857 3864 3867 3884 3895 3910 3911 3926 3931 3933 3938 3944 3945 3953 3954\n",
      " 3977 3979 3980 3984 3985 3998 3999 4003 4006 4008 4020 4022 4026 4029 4033\n",
      " 4035 4041 4048 4055 4070 4074 4077 4079 4082 4092 4093 4096 4107 4116 4118\n",
      " 4123 4125 4134 4144 4151 4159 4160 4161 4170 4171 4174 4188 4193 4196 4197\n",
      " 4198 4205 4211 4213 4216 4221 4228 4232 4242 4256 4258 4261 4263 4264 4271\n",
      " 4273 4274 4277 4287 4291 4298 4313 4316 4317 4325 4327 4328]\n",
      "TRAIN: [   0    1    3 ..., 4330 4331 4332] TEST: [   2    4    5    9   10   14   15   29   37   40   44   50   53   64   66\n",
      "   68   69   71   76   82   86   92   97   98  109  113  122  124  133  137\n",
      "  144  149  154  158  161  169  175  176  177  179  188  191  211  219  228\n",
      "  229  234  235  240  242  245  249  257  259  261  263  264  272  276  277\n",
      "  285  290  296  297  299  300  305  306  316  317  320  323  330  332  338\n",
      "  346  349  351  353  361  366  368  369  376  379  384  385  386  388  390\n",
      "  396  399  406  411  422  425  427  436  440  443  444  446  454  458  461\n",
      "  473  480  488  489  493  500  503  509  516  519  529  533  534  535  541\n",
      "  547  548  558  564  565  570  572  579  589  596  600  601  616  618  628\n",
      "  635  643  652  654  657  670  674  675  677  682  686  687  688  701  702\n",
      "  703  710  711  713  719  722  725  727  728  744  745  746  751  758  764\n",
      "  778  779  783  791  793  796  800  805  806  826  832  835  840  846  847\n",
      "  852  856  864  868  874  879  880  890  897  899  905  914  923  924  934\n",
      "  936  937  939  940  951  954  955  958  961  963  964  976  979  981  982\n",
      "  984  989  999 1003 1013 1014 1015 1018 1022 1023 1032 1036 1037 1038 1044\n",
      " 1047 1052 1056 1059 1069 1074 1077 1078 1087 1093 1094 1095 1100 1101 1104\n",
      " 1108 1110 1112 1122 1127 1128 1137 1139 1140 1150 1151 1157 1161 1170 1171\n",
      " 1174 1184 1195 1206 1220 1223 1225 1232 1235 1236 1237 1240 1245 1254 1256\n",
      " 1266 1268 1269 1277 1280 1283 1284 1285 1292 1299 1300 1310 1311 1317 1326\n",
      " 1330 1336 1341 1347 1359 1362 1363 1370 1371 1373 1376 1377 1379 1386 1388\n",
      " 1390 1397 1399 1411 1412 1414 1418 1422 1428 1446 1451 1460 1464 1465 1470\n",
      " 1471 1474 1475 1489 1493 1494 1496 1499 1501 1503 1505 1507 1513 1515 1519\n",
      " 1523 1526 1533 1535 1540 1547 1553 1555 1559 1567 1569 1573 1577 1581 1586\n",
      " 1601 1606 1618 1621 1642 1643 1649 1662 1666 1670 1680 1695 1697 1718 1728\n",
      " 1732 1736 1742 1746 1751 1752 1757 1759 1762 1764 1765 1766 1768 1774 1783\n",
      " 1790 1793 1794 1811 1813 1818 1819 1825 1835 1837 1839 1850 1852 1853 1855\n",
      " 1856 1861 1865 1867 1869 1872 1881 1892 1898 1900 1902 1905 1912 1914 1927\n",
      " 1928 1936 1939 1943 1944 1952 1957 1961 1969 1977 1979 1983 1991 1992 2004\n",
      " 2010 2011 2012 2019 2030 2031 2033 2044 2048 2049 2062 2063 2065 2066 2067\n",
      " 2094 2101 2104 2112 2118 2123 2137 2139 2142 2143 2146 2151 2154 2155 2160\n",
      " 2164 2165 2166 2174 2175 2180 2183 2200 2215 2219 2223 2226 2239 2242 2244\n",
      " 2252 2260 2270 2271 2284 2287 2293 2294 2306 2310 2323 2324 2332 2343 2347\n",
      " 2350 2355 2357 2361 2370 2372 2373 2377 2378 2382 2401 2404 2406 2412 2420\n",
      " 2439 2443 2455 2457 2462 2464 2472 2475 2478 2481 2487 2490 2497 2498 2505\n",
      " 2510 2519 2522 2526 2531 2533 2537 2539 2549 2553 2554 2563 2565 2569 2572\n",
      " 2575 2580 2583 2584 2587 2591 2600 2602 2606 2612 2613 2616 2617 2623 2629\n",
      " 2630 2632 2648 2657 2661 2663 2666 2668 2669 2670 2675 2693 2698 2707 2709\n",
      " 2724 2730 2740 2743 2744 2747 2749 2751 2752 2756 2759 2770 2777 2782 2793\n",
      " 2795 2801 2802 2806 2807 2808 2810 2812 2824 2826 2830 2838 2841 2843 2847\n",
      " 2853 2856 2864 2865 2869 2873 2877 2885 2886 2887 2888 2896 2898 2906 2916\n",
      " 2917 2918 2920 2921 2923 2929 2935 2941 2944 2950 2951 2954 2966 2969 2970\n",
      " 2972 2981 2986 2990 2996 3002 3004 3009 3010 3012 3014 3027 3032 3034 3037\n",
      " 3045 3061 3065 3066 3088 3090 3118 3119 3121 3126 3129 3130 3131 3133 3139\n",
      " 3149 3151 3167 3176 3179 3180 3185 3193 3203 3210 3213 3225 3227 3231 3234\n",
      " 3235 3242 3245 3256 3258 3260 3269 3276 3283 3284 3287 3295 3305 3308 3322\n",
      " 3323 3324 3328 3334 3341 3342 3347 3349 3357 3360 3366 3370 3371 3374 3378\n",
      " 3379 3380 3386 3391 3394 3406 3408 3409 3410 3411 3416 3423 3424 3436 3440\n",
      " 3443 3447 3448 3450 3457 3467 3470 3472 3477 3484 3485 3487 3488 3507 3512\n",
      " 3515 3520 3521 3522 3523 3524 3527 3542 3544 3549 3551 3552 3554 3572 3573\n",
      " 3576 3579 3586 3590 3598 3599 3613 3627 3628 3629 3637 3639 3643 3646 3652\n",
      " 3656 3660 3661 3669 3672 3676 3677 3693 3694 3702 3704 3710 3752 3753 3754\n",
      " 3755 3760 3763 3764 3768 3770 3775 3776 3780 3783 3785 3794 3795 3796 3808\n",
      " 3816 3817 3820 3829 3835 3844 3847 3849 3850 3859 3862 3865 3874 3876 3890\n",
      " 3892 3900 3901 3906 3915 3923 3924 3927 3934 3936 3937 3946 3949 3951 3958\n",
      " 3960 3982 4000 4012 4014 4016 4018 4024 4027 4043 4045 4049 4054 4057 4060\n",
      " 4062 4068 4069 4078 4090 4100 4109 4117 4124 4127 4131 4132 4136 4149 4153\n",
      " 4154 4155 4157 4162 4165 4169 4177 4185 4195 4200 4201 4218 4220 4237 4239\n",
      " 4241 4244 4246 4272 4279 4282 4289 4297 4310 4318 4320 4326]\n",
      "TRAIN: [   0    1    2 ..., 4329 4330 4331] TEST: [   6    8   11   13   18   20   27   41   43   47   48   52   55   58   59\n",
      "   67   77   80   84  101  106  107  108  119  121  128  132  135  139  141\n",
      "  145  147  152  157  166  187  190  194  198  200  204  206  215  232  233\n",
      "  238  241  244  250  251  254  255  260  262  267  268  269  270  271  274\n",
      "  278  279  283  287  294  301  303  309  310  312  314  315  318  328  340\n",
      "  342  344  354  357  359  360  391  392  393  394  413  414  426  431  438\n",
      "  441  442  455  456  459  468  474  475  476  477  478  486  487  490  505\n",
      "  506  507  511  512  513  518  522  532  557  563  568  574  584  585  588\n",
      "  590  593  597  602  609  610  619  633  642  646  653  655  665  667  672\n",
      "  676  689  700  715  717  721  723  731  733  743  752  757  762  766  767\n",
      "  772  774  781  784  789  792  795  810  812  813  815  818  819  820  822\n",
      "  828  831  838  849  850  855  859  862  865  876  878  884  886  892  893\n",
      "  904  906  909  912  913  916  921  926  942  943  946  966  969  970  972\n",
      "  980  986  987  988  993  995  997 1000 1010 1017 1025 1027 1030 1039 1041\n",
      " 1045 1049 1054 1055 1058 1061 1063 1067 1079 1081 1083 1085 1086 1096 1099\n",
      " 1103 1105 1106 1117 1136 1138 1142 1145 1147 1158 1165 1168 1173 1175 1182\n",
      " 1183 1187 1188 1190 1192 1196 1197 1199 1205 1210 1211 1212 1213 1215 1217\n",
      " 1222 1226 1231 1238 1244 1251 1252 1255 1257 1258 1260 1263 1264 1278 1293\n",
      " 1296 1301 1302 1304 1315 1318 1322 1323 1324 1335 1338 1339 1346 1352 1354\n",
      " 1355 1372 1374 1381 1382 1387 1393 1396 1403 1417 1424 1426 1444 1448 1449\n",
      " 1453 1454 1455 1458 1462 1463 1467 1476 1477 1485 1486 1512 1538 1539 1541\n",
      " 1543 1548 1549 1550 1570 1578 1587 1588 1590 1592 1599 1603 1604 1610 1611\n",
      " 1612 1617 1620 1626 1627 1633 1636 1637 1639 1644 1648 1663 1665 1673 1677\n",
      " 1678 1679 1683 1686 1693 1694 1709 1714 1717 1723 1724 1725 1735 1737 1739\n",
      " 1748 1749 1750 1753 1756 1758 1761 1763 1770 1771 1772 1773 1778 1779 1780\n",
      " 1785 1786 1806 1808 1815 1817 1822 1827 1840 1845 1846 1854 1857 1885 1886\n",
      " 1897 1901 1906 1915 1918 1920 1923 1925 1929 1930 1934 1947 1948 1958 1959\n",
      " 1962 1964 1966 1972 1974 1976 1978 1980 1981 1982 1994 1999 2000 2001 2005\n",
      " 2014 2024 2034 2038 2039 2043 2053 2054 2055 2064 2073 2074 2079 2082 2084\n",
      " 2090 2096 2109 2117 2128 2131 2132 2133 2140 2148 2158 2162 2172 2181 2184\n",
      " 2190 2201 2204 2205 2209 2213 2231 2235 2240 2241 2243 2248 2249 2265 2268\n",
      " 2278 2288 2297 2300 2302 2312 2319 2327 2330 2331 2342 2346 2359 2362 2368\n",
      " 2374 2379 2381 2385 2390 2399 2407 2408 2410 2415 2416 2424 2429 2430 2432\n",
      " 2433 2441 2447 2448 2460 2468 2476 2483 2486 2488 2491 2494 2501 2509 2513\n",
      " 2518 2530 2534 2540 2542 2552 2559 2560 2564 2570 2574 2585 2586 2588 2592\n",
      " 2594 2604 2605 2618 2625 2626 2627 2635 2640 2644 2650 2651 2655 2658 2667\n",
      " 2674 2677 2688 2689 2692 2699 2700 2702 2703 2704 2713 2714 2715 2717 2718\n",
      " 2720 2728 2735 2736 2741 2758 2765 2772 2776 2779 2781 2783 2784 2791 2797\n",
      " 2803 2804 2815 2828 2833 2855 2857 2858 2862 2866 2872 2875 2876 2879 2891\n",
      " 2892 2895 2901 2902 2907 2928 2933 2938 2940 2945 2948 2953 2958 2962 2964\n",
      " 2967 2968 2978 2979 2988 2992 2998 3011 3018 3019 3028 3029 3033 3035 3036\n",
      " 3039 3040 3042 3044 3050 3055 3059 3063 3068 3072 3074 3077 3081 3086 3093\n",
      " 3103 3106 3111 3112 3120 3135 3141 3147 3154 3161 3162 3164 3169 3171 3175\n",
      " 3178 3182 3188 3194 3195 3196 3202 3209 3218 3223 3228 3229 3237 3238 3241\n",
      " 3247 3249 3251 3254 3255 3257 3262 3267 3309 3311 3314 3320 3321 3327 3333\n",
      " 3350 3353 3359 3373 3376 3377 3381 3385 3388 3397 3401 3403 3417 3431 3439\n",
      " 3452 3459 3460 3462 3464 3478 3479 3490 3492 3499 3501 3504 3509 3517 3526\n",
      " 3535 3536 3537 3550 3553 3555 3558 3561 3563 3566 3574 3578 3594 3606 3607\n",
      " 3609 3619 3633 3647 3649 3651 3653 3659 3679 3683 3684 3688 3689 3701 3703\n",
      " 3705 3707 3718 3720 3726 3727 3728 3740 3741 3746 3747 3761 3769 3774 3777\n",
      " 3789 3799 3800 3802 3803 3806 3810 3813 3814 3824 3825 3828 3830 3836 3855\n",
      " 3861 3871 3873 3877 3878 3879 3882 3883 3894 3897 3899 3902 3904 3907 3908\n",
      " 3920 3922 3925 3930 3932 3935 3943 3947 3964 3970 3971 3974 3975 3981 3983\n",
      " 3988 3989 3990 3991 3992 3994 3995 4005 4007 4023 4030 4034 4039 4044 4046\n",
      " 4050 4056 4059 4061 4076 4085 4088 4097 4099 4104 4105 4106 4108 4129 4130\n",
      " 4139 4140 4152 4158 4167 4168 4175 4186 4189 4199 4206 4207 4210 4214 4215\n",
      " 4225 4230 4231 4233 4234 4236 4240 4243 4245 4247 4248 4267 4269 4270 4280\n",
      " 4281 4285 4286 4292 4293 4302 4304 4305 4312 4323 4324 4332]\n",
      "TRAIN: [   0    1    2 ..., 4328 4331 4332] TEST: [  16   19   32   35   38   51   54   56   60   65   73   75   78   79   81\n",
      "   83   89   91   96  102  103  110  111  114  123  129  131  140  143  155\n",
      "  165  171  172  178  183  184  186  205  208  210  212  213  230  231  239\n",
      "  243  247  258  265  273  293  313  319  324  335  336  339  341  355  356\n",
      "  358  362  363  364  367  370  371  372  373  374  377  382  395  397  401\n",
      "  412  416  420  421  432  434  435  439  445  448  450  464  466  471  479\n",
      "  484  494  495  498  510  523  524  539  540  542  549  551  552  554  561\n",
      "  571  580  583  608  611  613  625  626  631  632  644  645  649  656  658\n",
      "  660  662  679  681  693  695  697  718  724  726  729  737  740  749  750\n",
      "  759  760  761  763  765  769  771  782  785  787  790  798  801  808  811\n",
      "  814  823  833  839  844  853  858  863  875  877  881  887  889  901  907\n",
      "  911  915  919  930  947  948  957  960  974  978  992  994 1007 1008 1016\n",
      " 1019 1026 1029 1031 1034 1035 1050 1057 1060 1064 1075 1076 1080 1088 1089\n",
      " 1097 1102 1121 1123 1124 1125 1126 1132 1135 1143 1146 1153 1154 1155 1156\n",
      " 1164 1166 1178 1189 1193 1200 1216 1218 1227 1228 1229 1233 1234 1265 1274\n",
      " 1279 1291 1294 1295 1297 1307 1312 1316 1320 1321 1325 1340 1344 1345 1350\n",
      " 1356 1360 1361 1375 1378 1383 1391 1401 1405 1406 1407 1408 1415 1421 1435\n",
      " 1439 1440 1442 1456 1459 1466 1484 1492 1498 1500 1502 1506 1510 1516 1518\n",
      " 1520 1524 1527 1528 1534 1537 1557 1558 1565 1568 1575 1585 1591 1594 1595\n",
      " 1597 1598 1607 1609 1613 1614 1616 1619 1622 1624 1628 1630 1631 1635 1650\n",
      " 1655 1661 1664 1668 1674 1676 1681 1682 1689 1704 1706 1711 1716 1719 1727\n",
      " 1729 1730 1741 1743 1745 1776 1781 1782 1787 1788 1796 1801 1802 1803 1809\n",
      " 1814 1816 1820 1821 1833 1841 1842 1843 1848 1851 1858 1859 1860 1864 1866\n",
      " 1870 1873 1874 1876 1880 1882 1884 1890 1893 1903 1919 1921 1935 1937 1942\n",
      " 1946 1960 1968 1970 1984 1985 1988 1995 1996 1998 2007 2015 2018 2020 2026\n",
      " 2028 2029 2032 2035 2037 2045 2050 2056 2061 2069 2077 2083 2086 2092 2093\n",
      " 2097 2099 2113 2114 2116 2119 2122 2129 2130 2136 2138 2144 2145 2147 2149\n",
      " 2156 2159 2167 2168 2170 2177 2179 2202 2206 2207 2208 2218 2225 2228 2229\n",
      " 2233 2236 2238 2245 2246 2253 2259 2262 2264 2266 2267 2279 2286 2289 2291\n",
      " 2295 2303 2305 2309 2311 2313 2314 2316 2317 2320 2321 2325 2326 2328 2333\n",
      " 2338 2356 2360 2363 2364 2366 2376 2386 2388 2395 2397 2398 2402 2419 2421\n",
      " 2422 2426 2428 2449 2451 2454 2456 2458 2474 2479 2482 2495 2499 2503 2504\n",
      " 2512 2514 2515 2517 2520 2521 2524 2529 2532 2541 2548 2555 2557 2561 2568\n",
      " 2571 2576 2579 2581 2589 2596 2598 2614 2624 2634 2637 2638 2641 2642 2643\n",
      " 2647 2649 2652 2654 2656 2662 2680 2690 2695 2710 2716 2719 2725 2726 2733\n",
      " 2739 2746 2753 2754 2761 2769 2773 2780 2785 2789 2792 2794 2799 2800 2809\n",
      " 2817 2819 2821 2822 2827 2831 2832 2836 2837 2842 2845 2848 2851 2874 2878\n",
      " 2880 2883 2894 2910 2912 2915 2919 2922 2925 2927 2932 2936 2949 2952 2973\n",
      " 2982 2991 2993 2994 3006 3017 3020 3023 3030 3038 3041 3043 3047 3048 3052\n",
      " 3053 3054 3058 3062 3070 3087 3092 3096 3101 3102 3105 3108 3113 3115 3117\n",
      " 3124 3127 3132 3136 3137 3138 3142 3144 3152 3153 3156 3158 3163 3170 3172\n",
      " 3183 3189 3206 3215 3233 3239 3248 3259 3263 3265 3268 3270 3271 3272 3274\n",
      " 3275 3286 3288 3294 3296 3297 3302 3312 3316 3319 3335 3336 3338 3340 3343\n",
      " 3344 3345 3346 3348 3351 3354 3355 3358 3365 3375 3383 3392 3400 3402 3414\n",
      " 3427 3429 3430 3433 3437 3438 3442 3446 3449 3456 3471 3476 3480 3482 3489\n",
      " 3491 3493 3506 3508 3514 3518 3519 3525 3529 3530 3539 3541 3543 3545 3546\n",
      " 3557 3559 3562 3569 3582 3583 3584 3585 3589 3593 3597 3602 3603 3615 3616\n",
      " 3621 3634 3644 3650 3655 3658 3662 3663 3671 3675 3680 3681 3682 3685 3687\n",
      " 3697 3708 3711 3712 3713 3715 3719 3721 3724 3731 3733 3738 3743 3750 3756\n",
      " 3758 3767 3771 3772 3773 3782 3784 3790 3797 3815 3819 3827 3832 3837 3842\n",
      " 3845 3853 3856 3858 3863 3866 3868 3869 3875 3881 3885 3887 3888 3889 3891\n",
      " 3896 3903 3913 3916 3928 3939 3940 3941 3948 3950 3952 3955 3956 3961 3962\n",
      " 3963 3965 3966 3969 3972 3976 3978 3987 3993 3996 3997 4001 4010 4013 4015\n",
      " 4017 4019 4028 4032 4036 4037 4038 4042 4052 4058 4063 4064 4067 4073 4080\n",
      " 4081 4087 4094 4095 4102 4103 4111 4112 4113 4114 4115 4121 4128 4133 4137\n",
      " 4138 4141 4146 4147 4164 4172 4176 4179 4181 4190 4191 4192 4194 4202 4209\n",
      " 4217 4219 4223 4224 4250 4252 4254 4257 4265 4266 4275 4276 4283 4288 4290\n",
      " 4294 4300 4301 4303 4307 4309 4311 4319 4322 4329 4330]\n",
      "TRAIN: [   1    2    4 ..., 4329 4330 4332] TEST: [   0    3    7   12   21   24   25   26   28   46   62   63   74   90   93\n",
      "   94   95   99  100  105  112  115  116  120  126  127  136  146  150  151\n",
      "  153  160  163  164  167  168  174  180  181  193  197  199  201  207  209\n",
      "  216  222  225  236  237  246  256  266  275  280  281  282  284  288  292\n",
      "  307  321  325  327  329  331  334  337  343  348  350  365  375  387  389\n",
      "  400  403  404  407  409  417  419  423  424  428  430  433  437  447  449\n",
      "  451  460  462  469  492  497  504  508  515  525  537  544  550  560  573\n",
      "  577  586  591  592  594  595  603  604  605  606  612  614  617  622  623\n",
      "  624  627  630  637  640  647  650  659  664  669  671  673  680  691  694\n",
      "  696  698  699  705  707  709  714  730  732  734  736  738  739  741  742\n",
      "  747  753  754  755  756  770  777  780  786  797  802  804  807  816  821\n",
      "  824  827  830  834  837  845  848  851  854  857  860  869  870  873  885\n",
      "  894  902  903  908  920  922  928  929  931  932  941  945  950  952  956\n",
      "  959  967  968  973  975  990 1004 1006 1011 1020 1024 1028 1033 1040 1046\n",
      " 1048 1053 1066 1071 1072 1090 1092 1107 1111 1113 1130 1131 1133 1134 1149\n",
      " 1152 1159 1162 1163 1167 1169 1177 1179 1181 1186 1194 1198 1201 1202 1204\n",
      " 1207 1208 1209 1214 1219 1221 1241 1243 1247 1249 1250 1253 1286 1289 1298\n",
      " 1305 1306 1308 1309 1314 1331 1333 1337 1342 1348 1353 1365 1369 1384 1389\n",
      " 1392 1394 1395 1398 1400 1402 1404 1409 1413 1416 1425 1429 1433 1434 1437\n",
      " 1438 1441 1443 1447 1461 1468 1469 1472 1479 1480 1481 1483 1488 1497 1508\n",
      " 1514 1517 1522 1531 1532 1542 1545 1546 1551 1552 1556 1561 1562 1566 1574\n",
      " 1576 1579 1582 1584 1589 1596 1600 1605 1608 1623 1629 1634 1638 1640 1641\n",
      " 1645 1653 1659 1660 1667 1671 1672 1684 1685 1688 1690 1701 1705 1707 1720\n",
      " 1722 1731 1734 1740 1744 1747 1755 1769 1777 1792 1795 1797 1799 1800 1805\n",
      " 1807 1810 1812 1823 1834 1838 1844 1847 1849 1862 1863 1868 1871 1879 1883\n",
      " 1889 1891 1896 1904 1908 1909 1910 1911 1913 1924 1926 1932 1933 1938 1941\n",
      " 1954 1956 1963 1967 1971 1973 1975 1986 1987 1993 2002 2003 2006 2008 2009\n",
      " 2021 2022 2023 2040 2042 2046 2051 2059 2068 2080 2098 2105 2107 2120 2121\n",
      " 2127 2135 2150 2152 2153 2157 2163 2173 2176 2182 2185 2186 2187 2191 2195\n",
      " 2197 2199 2214 2217 2220 2221 2222 2227 2237 2247 2251 2256 2257 2261 2269\n",
      " 2273 2274 2276 2282 2283 2290 2292 2296 2298 2304 2322 2335 2336 2341 2344\n",
      " 2349 2352 2353 2354 2371 2375 2380 2384 2387 2389 2391 2392 2393 2394 2400\n",
      " 2403 2405 2413 2414 2417 2418 2431 2435 2437 2440 2442 2444 2446 2461 2463\n",
      " 2467 2469 2471 2477 2489 2492 2496 2502 2508 2511 2516 2536 2538 2544 2546\n",
      " 2547 2550 2562 2582 2593 2595 2599 2603 2607 2608 2609 2610 2633 2636 2639\n",
      " 2646 2659 2660 2665 2671 2676 2679 2681 2683 2684 2685 2687 2691 2711 2712\n",
      " 2721 2723 2731 2732 2738 2742 2745 2748 2750 2757 2763 2766 2767 2775 2778\n",
      " 2786 2787 2798 2811 2813 2814 2820 2834 2850 2852 2860 2863 2867 2870 2881\n",
      " 2882 2884 2893 2897 2899 2900 2903 2904 2908 2924 2930 2934 2937 2956 2957\n",
      " 2960 2963 2965 2974 2977 2987 2989 2995 2997 2999 3000 3024 3026 3046 3056\n",
      " 3057 3060 3064 3067 3078 3085 3089 3091 3107 3109 3110 3114 3116 3123 3125\n",
      " 3128 3134 3143 3146 3148 3150 3165 3166 3168 3186 3187 3190 3191 3192 3200\n",
      " 3204 3205 3207 3208 3216 3219 3220 3221 3224 3226 3230 3243 3246 3252 3264\n",
      " 3273 3277 3278 3279 3280 3281 3282 3285 3289 3290 3291 3292 3298 3299 3300\n",
      " 3306 3310 3315 3317 3318 3330 3331 3337 3356 3361 3362 3372 3390 3393 3395\n",
      " 3396 3398 3404 3413 3418 3419 3420 3421 3425 3426 3432 3441 3444 3445 3451\n",
      " 3454 3455 3461 3463 3465 3466 3468 3483 3486 3505 3510 3511 3513 3516 3531\n",
      " 3533 3547 3548 3560 3568 3570 3575 3580 3588 3591 3596 3604 3610 3611 3612\n",
      " 3622 3623 3630 3631 3635 3636 3638 3642 3645 3654 3657 3665 3666 3674 3678\n",
      " 3686 3692 3695 3698 3706 3716 3717 3723 3729 3730 3732 3734 3742 3745 3748\n",
      " 3749 3751 3757 3762 3765 3766 3778 3781 3791 3792 3793 3801 3804 3811 3821\n",
      " 3834 3839 3846 3851 3854 3860 3870 3872 3880 3886 3893 3898 3905 3909 3912\n",
      " 3914 3917 3918 3919 3921 3929 3942 3957 3959 3967 3968 3973 3986 4002 4004\n",
      " 4009 4011 4021 4025 4031 4040 4047 4051 4053 4065 4066 4071 4072 4075 4083\n",
      " 4084 4086 4089 4091 4098 4101 4110 4119 4120 4122 4126 4135 4142 4143 4145\n",
      " 4148 4150 4156 4163 4166 4173 4178 4180 4182 4183 4184 4187 4203 4204 4208\n",
      " 4212 4222 4226 4227 4229 4235 4238 4249 4251 4253 4255 4259 4260 4262 4268\n",
      " 4278 4284 4295 4296 4299 4306 4308 4314 4315 4321 4331]\n"
     ]
    }
   ],
   "source": [
    "# k-fold Splitting\n",
    "data_x = np.array(full_data[sense_word][0])\n",
    "data_y = np.array(full_data[sense_word][2])\n",
    "kf = KFold(n_splits=5,shuffle=True,random_state=0)\n",
    "\n",
    "for train_index, test_index in kf.split(data_x):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    #x_train, x_test = data_x[train_index], data_x[test_index]\n",
    "    #y_train, y_test = data_y[train_index], data_y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/btech/aviraj/cs771/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Splitting\n",
    "data_x = full_data[sense_word][0]\n",
    "data_y = full_data[sense_word][2]\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, train_size=0.8, shuffle=True, stratify=data_y,random_state=0)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.9, shuffle=True, stratify=y_train,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_prepare(x):\n",
    "    num_examples = len(x)\n",
    "\n",
    "    xx = np.zeros([num_examples, max_sent_size], dtype=int)\n",
    "    xx_mask = np.zeros([num_examples, max_sent_size], dtype=bool)\n",
    "\n",
    "    for j in range(num_examples):\n",
    "        for i in range(max_sent_size):\n",
    "            if(i>=len(x[j])):\n",
    "                break\n",
    "            w = x[j][i]\n",
    "            xx[j][i] = word2id[w] if w in word2id else word2id['UNKNOWN_TOKEN']\n",
    "            xx_mask[j][i] = True\n",
    "            \n",
    "    return xx, xx_mask\n",
    "\n",
    "def eval_f1(yy, pred):\n",
    "    num_batches = int(len(yy)/batch_size)\n",
    "    f1 = f1_score(yy[:batch_size*num_batches], pred, average='macro')\n",
    "    return f1*100\n",
    "\n",
    "def model(xx, yy, mask, train_cond=True):\n",
    "    num_batches = int(len(xx)/batch_size)\n",
    "    losses = 0\n",
    "    preds = []\n",
    "    for j in range(num_batches): \n",
    "        \n",
    "        s = j * batch_size\n",
    "        e = (j+1) * batch_size\n",
    "        \n",
    "        feed_dict = {x:xx[s:e], y:yy[s:e], x_mask:mask[s:e], is_train:train_cond, input_keep_prob:keep_prob, word_emb_mat:word_embedding}\n",
    "        \n",
    "        \n",
    "        if(train_cond==True):\n",
    "            _, _loss, step, _summary = sess.run([train_op, total_loss, global_step, summary], feed_dict)\n",
    "            summary_writer.add_summary(_summary, step)  \n",
    "#             print(\"Steps:{}\".format(step), \", Loss: {}\".format(_loss))\n",
    "\n",
    "        else:\n",
    "            _loss, pred = sess.run([total_loss, predictions], feed_dict)\n",
    "            preds.append(pred)\n",
    "            \n",
    "        losses +=_loss\n",
    "\n",
    "    if(train_cond==False):\n",
    "        y_pred = []\n",
    "        for i in range(num_batches):\n",
    "            for pred in preds[i]:\n",
    "                y_pred.append(pred)\n",
    "        return losses/num_batches, y_pred\n",
    "    \n",
    "    return losses/num_batches, step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_id_train, mask_train = data_prepare(x_train)\n",
    "x_id_val, mask_val = data_prepare(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "#     random = np.random.choice(num_train_data, size=(num_train_data), replace=False)\n",
    "#     X_train = train_x[random]\n",
    "#     Y_train = train_y[random]\n",
    "#     X_mask_train = train_x_mask[random]\n",
    "            \n",
    "    losses, step = model(x_id_train, y_train, mask_train)\n",
    "    print(\"Epoch:\", i+1,\"Step:\", step, \"loss:\",losses)\n",
    "    saver.save(sess, save_path=save_dir)                         \n",
    "    print(\"Saved Model Complete\")\n",
    "    val_loss, val_pred = model(x_id_val, y_val, mask_val, train_cond=False)\n",
    "    print(\"Validation F1 Score: \",  eval_f1(y_val, val_pred), \"Loss: \", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "#     random = np.random.choice(num_train_data, size=(num_train_data), replace=False)\n",
    "#     X_train = train_x[random]\n",
    "#     Y_train = train_y[random]\n",
    "#     X_mask_train = train_x_mask[random]\n",
    "            \n",
    "    losses, step = model(x_id_train, y_train, mask_train)\n",
    "    print(\"Epoch:\", i+1,\"Step:\", step, \"loss:\",losses)\n",
    "    saver.save(sess, save_path=save_dir)                         \n",
    "    print(\"Saved Model Complete\")\n",
    "    val_loss, val_pred = model(x_id_val, y_val, mask_val, train_cond=False)\n",
    "    print(\"Validation F1 Score: \",  eval_f1(y_val, val_pred), \"Loss: \", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "#     random = np.random.choice(num_train_data, size=(num_train_data), replace=False)\n",
    "#     X_train = train_x[random]\n",
    "#     Y_train = train_y[random]\n",
    "#     X_mask_train = train_x_mask[random]\n",
    "            \n",
    "    losses, step = model(x_id_train, y_train, mask_train)\n",
    "    print(\"Epoch:\", i+1,\"Step:\", step, \"loss:\",losses)\n",
    "    saver.save(sess, save_path=save_dir)                         \n",
    "    print(\"Saved Model Complete\")\n",
    "    val_loss, val_pred = model(x_id_val, y_val, mask_val, train_cond=False)\n",
    "    print(\"Validation F1 Score: \",  eval_f1(y_val, val_pred), \"Loss: \", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "#     random = np.random.choice(num_train_data, size=(num_train_data), replace=False)\n",
    "#     X_train = train_x[random]\n",
    "#     Y_train = train_y[random]\n",
    "#     X_mask_train = train_x_mask[random]\n",
    "            \n",
    "    losses, step = model(x_id_train, y_train, mask_train)\n",
    "    print(\"Epoch:\", i+1,\"Step:\", step, \"loss:\",losses)\n",
    "    saver.save(sess, save_path=save_dir)                         \n",
    "    print(\"Saved Model Complete\")\n",
    "    val_loss, val_pred = model(x_id_val, y_val, mask_val, train_cond=False)\n",
    "    print(\"Validation F1 Score: \",  eval_f1(y_val, val_pred), \"Loss: \", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "#     random = np.random.choice(num_train_data, size=(num_train_data), replace=False)\n",
    "#     X_train = train_x[random]\n",
    "#     Y_train = train_y[random]\n",
    "#     X_mask_train = train_x_mask[random]\n",
    "            \n",
    "    losses, step = model(x_id_train, y_train, mask_train)\n",
    "    print(\"Epoch:\", i+1,\"Step:\", step, \"loss:\",losses)\n",
    "    saver.save(sess, save_path=save_dir)                         \n",
    "    print(\"Saved Model Complete\")\n",
    "    val_loss, val_pred = model(x_id_val, y_val, mask_val, train_cond=False)\n",
    "    print(\"Validation F1 Score: \",  eval_f1(y_val, val_pred), \"Loss: \", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "saver.restore(sess, tf.train.latest_checkpoint(save_dir))\n",
    "embed_mat = sess.run(embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs771",
   "language": "python",
   "name": "cs771"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
