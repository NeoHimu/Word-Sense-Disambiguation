{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robsr/miniconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../Glove/word_embedding_glove', 'rb')\n",
    "word_embedding = pickle.load(f)\n",
    "f.close()\n",
    "word_embedding = word_embedding[: len(word_embedding)-1]\n",
    "\n",
    "f = open('../Glove/vocab_glove', 'rb')\n",
    "vocab = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "word2id = dict((w, i) for i,w in enumerate(vocab))\n",
    "id2word = dict((i, w) for i,w in enumerate(vocab))\n",
    "\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "\n",
    "f = open(\"train.pickle\", 'rb')\n",
    "full_data = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Description\n",
    "sense_word = 'hard'\n",
    "model_name = 'basic2'\n",
    "model_dir = 'output/' + sense_word + '/' + model_name\n",
    "save_dir = os.path.join(model_dir, \"save/\")\n",
    "log_dir = os.path.join(model_dir, \"log\")\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "mode = 'train'\n",
    "num_senses = 3\n",
    "batch_size = 64\n",
    "vocab_size = len(vocab)\n",
    "unk_vocab_size = 1\n",
    "word_emb_size = len(word_embedding[0])\n",
    "max_sent_size = 200\n",
    "hidden_size = 100\n",
    "keep_prob = 0.5\n",
    "l2_lambda = 0.001\n",
    "init_lr = 0.01\n",
    "decay_steps = 500\n",
    "decay_rate = 0.96\n",
    "clip_norm = 1\n",
    "clipping = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "x = tf.placeholder('int32', [batch_size, max_sent_size], name=\"x\")\n",
    "y = tf.placeholder('int32', [batch_size], name=\"y\")\n",
    "x_mask  = tf.placeholder('bool', [batch_size, max_sent_size], name='x_mask') \n",
    "is_train = tf.placeholder('bool', [], name='is_train')\n",
    "word_emb_mat = tf.placeholder('float', [None, word_emb_size], name='emb_mat')\n",
    "input_keep_prob = tf.cond(is_train,lambda:keep_prob, lambda:tf.constant(1.0))\n",
    "x_len = tf.reduce_sum(tf.cast(x_mask, 'int32'), 1)\n",
    "\n",
    "with tf.name_scope(\"word_embedding\"):\n",
    "    if mode == 'train':\n",
    "        unk_word_emb_mat = tf.get_variable(\"word_emb_mat\", dtype='float', shape=[unk_vocab_size, word_emb_size], initializer=tf.contrib.layers.xavier_initializer(uniform=False, seed=0, dtype=tf.float32))\n",
    "    else:\n",
    "        unk_word_emb_mat = tf.get_variable(\"word_emb_mat\", shape=[unk_vocab_size, word_emb_size], dtype='float')\n",
    "        \n",
    "    final_word_emb_mat = tf.concat([word_emb_mat, unk_word_emb_mat], 0)\n",
    "    Wx = tf.nn.embedding_lookup(final_word_emb_mat, x)  \n",
    "\n",
    "with tf.variable_scope(\"lstm1\"):\n",
    "    cell_fw1 = tf.contrib.rnn.BasicLSTMCell(hidden_size,state_is_tuple=True)\n",
    "    cell_bw1 = tf.contrib.rnn.BasicLSTMCell(hidden_size,state_is_tuple=True)\n",
    "\n",
    "    d_cell_fw1 = tf.contrib.rnn.DropoutWrapper(cell_fw1, input_keep_prob=input_keep_prob)\n",
    "    d_cell_bw1 = tf.contrib.rnn.DropoutWrapper(cell_bw1, input_keep_prob=input_keep_prob)\n",
    "    \n",
    "    (fw_h1, bw_h1), _ = tf.nn.bidirectional_dynamic_rnn(d_cell_fw1, d_cell_bw1, Wx, sequence_length=x_len, dtype='float', scope='lstm1')\n",
    "    h1 = tf.concat([fw_h1, bw_h1], 2)\n",
    "    \n",
    "with tf.variable_scope(\"lstm2\"):\n",
    "    cell_fw2 = tf.contrib.rnn.BasicLSTMCell(hidden_size,state_is_tuple=True)\n",
    "    cell_bw2 = tf.contrib.rnn.BasicLSTMCell(hidden_size,state_is_tuple=True)\n",
    "\n",
    "    d_cell_fw2 = tf.contrib.rnn.DropoutWrapper(cell_fw2, input_keep_prob=input_keep_prob)\n",
    "    d_cell_bw2 = tf.contrib.rnn.DropoutWrapper(cell_bw2, input_keep_prob=input_keep_prob)\n",
    "    \n",
    "    (fw_h2, bw_h2), _ = tf.nn.bidirectional_dynamic_rnn(d_cell_fw2, d_cell_bw2, h1, sequence_length=x_len, dtype='float', scope='lstm2')\n",
    "    h = tf.concat([fw_h2, bw_h2], 2)\n",
    "\n",
    "def attention(input_x, input_mask, W_att):\n",
    "    h_masked = tf.boolean_mask(input_x, input_mask)\n",
    "    h_tanh = tf.tanh(h_masked)\n",
    "    u = tf.matmul(h_tanh, W_att)\n",
    "    a = tf.nn.softmax(u)\n",
    "    c = tf.reduce_sum(tf.multiply(h_tanh, a), 0)  \n",
    "    return c\n",
    "\n",
    "with tf.variable_scope(\"attention\"):\n",
    "    W_att = tf.get_variable(\"W_att\", shape = [2*hidden_size, 1], initializer = tf.contrib.layers.xavier_initializer(uniform=False, seed=0, dtype=tf.float32))\n",
    "    c = tf.expand_dims(attention(h[0], x_mask[0], W_att), 0)\n",
    "    for i in range(1, batch_size):\n",
    "        c = tf.concat([c, tf.expand_dims(attention(h[i], x_mask[i], W_att), 0)], 0)\n",
    "\n",
    "with tf.variable_scope(\"fc_layer\"):\n",
    "    W_fc = tf.get_variable(\"W_fc\", shape = [2*hidden_size, 2*hidden_size], initializer = tf.contrib.layers.xavier_initializer(uniform=False, seed=0, dtype=tf.float32))\n",
    "    b_fc = tf.Variable(tf.zeros([2*hidden_size]), name=\"b_fc\")\n",
    "    drop_c = tf.nn.dropout(c, input_keep_prob)\n",
    "    logits = tf.matmul(drop_c, W_fc) + b_fc\n",
    "    predictions = tf.argmax(logits, 1)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(init_lr, global_step, decay_steps, decay_rate, staircase=True)\n",
    "\n",
    "tv_all = tf.trainable_variables()\n",
    "tv_regu =[]\n",
    "for t in tv_all:\n",
    "    if t.name.find('b:')==-1:\n",
    "        tv_regu.append(t)\n",
    "        \n",
    "# l2 Loss\n",
    "l2_loss = l2_lambda * tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv_regu ])\n",
    "\n",
    "total_loss = loss + l2_loss\n",
    "\n",
    "# Optimizer for loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "# Gradients and Variables for Loss\n",
    "grads_vars = optimizer.compute_gradients(total_loss)\n",
    "\n",
    "# Clipping of Gradients\n",
    "clipped_grads = grads_vars\n",
    "if(clipping == True):\n",
    "    clipped_grads = [(tf.clip_by_norm(grad, clip_norm), var) for grad, var in clipped_grads]\n",
    "\n",
    "# Training Optimizer for Total Loss\n",
    "train_op = optimizer.apply_gradients(clipped_grads, global_step=global_step)\n",
    "\n",
    "# Summaries\n",
    "var_summaries = []\n",
    "for v in tv_all:\n",
    "    var_summary = tf.summary.histogram(\"{}/var\".format(v.name), v)\n",
    "    var_summaries.append(var_summary)\n",
    "\n",
    "var_summaries_merged = tf.summary.merge(var_summaries)\n",
    "\n",
    "loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "total_loss_summary = tf.summary.scalar(\"total_loss\", total_loss)\n",
    "summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "sess.run(tf.global_variables_initializer())                          # For initializing all the variables\n",
    "saver = tf.train.Saver()                                             # For Saving the model\n",
    "summary_writer = tf.summary.FileWriter(log_dir, sess.graph)          # For writing Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robsr/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2772, 694)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting\n",
    "data_x = full_data[sense_word][0]\n",
    "data_y = full_data[sense_word][2]\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, train_size=0.8, shuffle=True, stratify=data_y, random_state=0)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.8, shuffle=True, stratify=y_train, random_state=0)\n",
    "\n",
    "len(x_train), len(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare(x):\n",
    "    num_examples = len(x)\n",
    "\n",
    "    xx = np.zeros([num_examples, max_sent_size], dtype=int)\n",
    "    xx_mask = np.zeros([num_examples, max_sent_size], dtype=bool)\n",
    "\n",
    "    for j in range(num_examples):\n",
    "        for i in range(max_sent_size):\n",
    "            if(i>=len(x[j])):\n",
    "                break\n",
    "            w = x[j][i]\n",
    "            xx[j][i] = word2id[w] if w in word2id else word2id['UNKNOWN_TOKEN']\n",
    "            xx_mask[j][i] = True\n",
    "            \n",
    "    return xx, xx_mask\n",
    "\n",
    "def eval_score(yy, pred):\n",
    "    num_batches = int(len(yy)/batch_size)\n",
    "    f1 = f1_score(yy[:batch_size*num_batches], pred, average='macro')\n",
    "    accu = accuracy_score(yy[:batch_size*num_batches], pred)\n",
    "    return f1*100, accu*100\n",
    "\n",
    "def model(xx, yy, mask, train_cond=True):\n",
    "    num_batches = int(len(xx)/batch_size)\n",
    "    losses = 0\n",
    "    preds = []\n",
    "    for j in range(num_batches): \n",
    "        \n",
    "        s = j * batch_size\n",
    "        e = (j+1) * batch_size\n",
    "        \n",
    "        feed_dict = {x:xx[s:e], y:yy[s:e], x_mask:mask[s:e], is_train:train_cond, input_keep_prob:keep_prob, word_emb_mat:word_embedding}\n",
    "        \n",
    "        \n",
    "        if(train_cond==True):\n",
    "            _, _loss, step, _summary = sess.run([train_op, total_loss, global_step, summary], feed_dict)\n",
    "            summary_writer.add_summary(_summary, step)\n",
    "            if step%5 == 0:\n",
    "                print(\"Steps:{}\".format(step), \", Loss: {}\".format(_loss))\n",
    "        else:\n",
    "            _loss, pred = sess.run([total_loss, predictions], feed_dict)\n",
    "            preds.append(pred)\n",
    "            \n",
    "        losses +=_loss\n",
    "\n",
    "    if(train_cond==False):\n",
    "        y_pred = []\n",
    "        for i in range(num_batches):\n",
    "            for pred in preds[i]:\n",
    "                y_pred.append(pred)\n",
    "        return losses/num_batches, y_pred\n",
    "    \n",
    "    return losses/num_batches, step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_id_train, mask_train = data_prepare(x_train)\n",
    "x_id_val, mask_val = data_prepare(x_val)\n",
    "x_id_test, mask_test = data_prepare(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps:5 , Loss: 6.258545875549316\n",
      "Steps:10 , Loss: 3.520009994506836\n",
      "Steps:15 , Loss: 2.5930826663970947\n",
      "Steps:20 , Loss: 1.8873353004455566\n",
      "Steps:25 , Loss: 1.9703603982925415\n",
      "Steps:30 , Loss: 1.5365471839904785\n",
      "Steps:35 , Loss: 1.5063180923461914\n",
      "Steps:40 , Loss: 1.4787731170654297\n",
      "Epoch: 1 Step: 43 loss: 4.24284016254\n",
      "Steps:45 , Loss: 1.9885308742523193\n",
      "Steps:50 , Loss: 1.308830738067627\n",
      "Steps:55 , Loss: 1.0597845315933228\n",
      "Steps:60 , Loss: 1.0756244659423828\n",
      "Steps:65 , Loss: 0.9970892667770386\n",
      "Steps:70 , Loss: 0.9558264017105103\n",
      "Steps:75 , Loss: 0.9669712781906128\n",
      "Steps:80 , Loss: 0.9953579902648926\n",
      "Steps:85 , Loss: 0.9534716606140137\n",
      "Epoch: 2 Step: 86 loss: 1.11134817988\n",
      "Train: F1 Score:  34.1266621591 Accuracy:  80.3779069767 Loss:  0.969300739987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robsr/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: F1 Score:  31.9478206937 Accuracy:  80.15625 Loss:  0.954839593172\n",
      "Steps:90 , Loss: 0.9774026870727539\n",
      "Steps:95 , Loss: 0.778002142906189\n",
      "Steps:100 , Loss: 0.9419623613357544\n",
      "Steps:105 , Loss: 1.215635895729065\n",
      "Steps:110 , Loss: 0.8069422245025635\n",
      "Steps:115 , Loss: 0.7736497521400452\n",
      "Steps:120 , Loss: 0.7267675399780273\n",
      "Steps:125 , Loss: 0.7702867388725281\n",
      "Epoch: 3 Step: 129 loss: 0.860231288644\n",
      "Steps:130 , Loss: 0.7471674680709839\n",
      "Steps:135 , Loss: 0.7986025214195251\n",
      "Steps:140 , Loss: 1.336484432220459\n",
      "Steps:145 , Loss: 1.0848180055618286\n",
      "Steps:150 , Loss: 0.6263355016708374\n",
      "Steps:155 , Loss: 0.7151857614517212\n",
      "Steps:160 , Loss: 0.8212499618530273\n",
      "Steps:165 , Loss: 0.6217370629310608\n",
      "Steps:170 , Loss: 0.4638786315917969\n",
      "Epoch: 4 Step: 172 loss: 0.793579295624\n",
      "Train: F1 Score:  52.7082053696 Accuracy:  83.1395348837 Loss:  0.72580055442\n",
      "Val: F1 Score:  51.6086445854 Accuracy:  82.03125 Loss:  0.789165437222\n",
      "Steps:175 , Loss: 0.7286161184310913\n",
      "Steps:180 , Loss: 0.6882705688476562\n",
      "Steps:185 , Loss: 0.44545620679855347\n",
      "Steps:190 , Loss: 0.6221297979354858\n",
      "Steps:195 , Loss: 0.7327039241790771\n",
      "Steps:200 , Loss: 0.6077964305877686\n",
      "Steps:205 , Loss: 0.6009422540664673\n",
      "Steps:210 , Loss: 0.5874171257019043\n",
      "Steps:215 , Loss: 0.6657791137695312\n",
      "Epoch: 5 Step: 215 loss: 0.714410432549\n",
      "Steps:220 , Loss: 0.6893134713172913\n",
      "Steps:225 , Loss: 0.639703631401062\n",
      "Steps:230 , Loss: 0.5827658176422119\n",
      "Steps:235 , Loss: 0.5747667551040649\n",
      "Steps:240 , Loss: 0.5821142196655273\n",
      "Steps:245 , Loss: 0.6828800439834595\n",
      "Steps:250 , Loss: 0.5338796377182007\n",
      "Steps:255 , Loss: 0.6616832613945007\n",
      "Epoch: 6 Step: 258 loss: 0.673880040646\n",
      "Train: F1 Score:  69.1964945277 Accuracy:  82.3037790698 Loss:  0.701807580715\n",
      "Val: F1 Score:  76.9013705594 Accuracy:  85.9375 Loss:  0.675496280193\n",
      "Steps:260 , Loss: 0.6495156288146973\n",
      "Steps:265 , Loss: 0.9066476821899414\n",
      "Steps:270 , Loss: 0.6979552507400513\n",
      "Steps:275 , Loss: 0.4634890854358673\n",
      "Steps:280 , Loss: 0.7356204986572266\n",
      "Steps:285 , Loss: 0.6324180364608765\n",
      "Steps:290 , Loss: 0.5617921352386475\n",
      "Steps:295 , Loss: 0.5927984714508057\n",
      "Steps:300 , Loss: 0.7689613699913025\n",
      "Epoch: 7 Step: 301 loss: 0.655263402434\n",
      "Steps:305 , Loss: 0.6768361330032349\n",
      "Steps:310 , Loss: 0.5814148187637329\n",
      "Steps:315 , Loss: 0.6810815334320068\n",
      "Steps:320 , Loss: 0.46484607458114624\n",
      "Steps:325 , Loss: 0.7370208501815796\n",
      "Steps:330 , Loss: 0.9233638048171997\n",
      "Steps:335 , Loss: 0.7916808724403381\n",
      "Steps:340 , Loss: 0.5918325185775757\n",
      "Epoch: 8 Step: 344 loss: 0.643469211667\n",
      "Train: F1 Score:  77.3118985016 Accuracy:  88.4447674419 Loss:  0.615455014761\n",
      "Val: F1 Score:  78.4303175145 Accuracy:  88.75 Loss:  0.623991525173\n",
      "Steps:345 , Loss: 0.5001140832901001\n",
      "Steps:350 , Loss: 0.7959595322608948\n",
      "Steps:355 , Loss: 0.7360042333602905\n",
      "Steps:360 , Loss: 0.5436484217643738\n",
      "Steps:365 , Loss: 0.6293537616729736\n",
      "Steps:370 , Loss: 0.6721868515014648\n",
      "Steps:375 , Loss: 0.6047725677490234\n",
      "Steps:380 , Loss: 0.6141512989997864\n",
      "Steps:385 , Loss: 0.522813081741333\n",
      "Epoch: 9 Step: 387 loss: 0.632553593364\n",
      "Steps:390 , Loss: 0.6710823774337769\n",
      "Steps:395 , Loss: 0.5688897371292114\n",
      "Steps:400 , Loss: 0.482344388961792\n",
      "Steps:405 , Loss: 0.5663244128227234\n",
      "Steps:410 , Loss: 0.634844958782196\n",
      "Steps:415 , Loss: 0.5529918670654297\n",
      "Steps:420 , Loss: 0.7358548045158386\n",
      "Steps:425 , Loss: 0.4841427505016327\n",
      "Steps:430 , Loss: 0.4409065544605255\n",
      "Epoch: 10 Step: 430 loss: 0.635817260936\n",
      "Train: F1 Score:  73.0618581436 Accuracy:  88.1904069767 Loss:  0.641697162806\n",
      "Val: F1 Score:  74.3985776519 Accuracy:  88.90625 Loss:  0.728432878852\n",
      "Steps:435 , Loss: 0.5206866264343262\n",
      "Steps:440 , Loss: 0.5086746215820312\n",
      "Steps:445 , Loss: 0.6871547698974609\n",
      "Steps:450 , Loss: 0.5752023458480835\n",
      "Steps:455 , Loss: 0.7712229490280151\n",
      "Steps:460 , Loss: 0.4161064624786377\n",
      "Steps:465 , Loss: 0.7899056673049927\n",
      "Steps:470 , Loss: 0.5169498920440674\n",
      "Epoch: 11 Step: 473 loss: 0.625473898511\n",
      "Steps:475 , Loss: 0.987274706363678\n",
      "Steps:480 , Loss: 0.5467910170555115\n",
      "Steps:485 , Loss: 0.8074840307235718\n",
      "Steps:490 , Loss: 1.0281875133514404\n",
      "Steps:495 , Loss: 0.6273869276046753\n",
      "Steps:500 , Loss: 0.5483580827713013\n",
      "Steps:505 , Loss: 0.5840213298797607\n",
      "Steps:510 , Loss: 0.6108313798904419\n",
      "Steps:515 , Loss: 0.6744440793991089\n",
      "Epoch: 12 Step: 516 loss: 0.637404355199\n",
      "Train: F1 Score:  79.6111817853 Accuracy:  90.9520348837 Loss:  0.556479476219\n",
      "Val: F1 Score:  77.1960237805 Accuracy:  89.21875 Loss:  0.621214097738\n",
      "Steps:520 , Loss: 0.8332123160362244\n",
      "Steps:525 , Loss: 0.6354844570159912\n",
      "Steps:530 , Loss: 0.6412856578826904\n",
      "Steps:535 , Loss: 0.5903709530830383\n",
      "Steps:540 , Loss: 0.5241371989250183\n",
      "Steps:545 , Loss: 0.3612348735332489\n",
      "Steps:550 , Loss: 0.6619349718093872\n",
      "Steps:555 , Loss: 0.5917000770568848\n",
      "Epoch: 13 Step: 559 loss: 0.613791179518\n",
      "Steps:560 , Loss: 0.7359122037887573\n",
      "Steps:565 , Loss: 0.5167019367218018\n",
      "Steps:570 , Loss: 0.5451569557189941\n",
      "Steps:575 , Loss: 0.4534146189689636\n",
      "Steps:580 , Loss: 0.6657294034957886\n",
      "Steps:585 , Loss: 0.462177574634552\n",
      "Steps:590 , Loss: 0.5040395855903625\n",
      "Steps:595 , Loss: 0.47163599729537964\n",
      "Steps:600 , Loss: 0.5969127416610718\n",
      "Epoch: 14 Step: 602 loss: 0.563560546138\n",
      "Train: F1 Score:  78.2261389685 Accuracy:  90.4433139535 Loss:  0.569183040497\n",
      "Val: F1 Score:  71.0440240366 Accuracy:  87.96875 Loss:  0.65373506844\n",
      "Steps:605 , Loss: 0.45061561465263367\n",
      "Steps:610 , Loss: 0.5751817226409912\n",
      "Steps:615 , Loss: 0.5411821007728577\n",
      "Steps:620 , Loss: 0.6059495210647583\n",
      "Steps:625 , Loss: 0.5824214220046997\n",
      "Steps:630 , Loss: 0.5141491889953613\n",
      "Steps:635 , Loss: 0.4454408288002014\n",
      "Steps:640 , Loss: 0.6578733325004578\n",
      "Steps:645 , Loss: 0.5817145109176636\n",
      "Epoch: 15 Step: 645 loss: 0.617732549823\n",
      "Steps:650 , Loss: 0.4445582628250122\n",
      "Steps:655 , Loss: 0.6676060557365417\n",
      "Steps:660 , Loss: 0.5838647484779358\n",
      "Steps:665 , Loss: 0.8176352381706238\n",
      "Steps:670 , Loss: 0.49966126680374146\n",
      "Steps:675 , Loss: 0.6519149541854858\n",
      "Steps:680 , Loss: 0.45420655608177185\n",
      "Steps:685 , Loss: 0.6697636246681213\n",
      "Epoch: 16 Step: 688 loss: 0.599957240182\n",
      "Train: F1 Score:  83.9119313542 Accuracy:  92.1148255814 Loss:  0.54243456347\n",
      "Val: F1 Score:  81.0827428349 Accuracy:  90.625 Loss:  0.57386918664\n",
      "Steps:690 , Loss: 0.7706722021102905\n",
      "Steps:695 , Loss: 0.7202504873275757\n",
      "Steps:700 , Loss: 0.4520285129547119\n",
      "Steps:705 , Loss: 0.5863051414489746\n",
      "Steps:710 , Loss: 0.8076704144477844\n",
      "Steps:715 , Loss: 0.5560353994369507\n",
      "Steps:720 , Loss: 0.6885330677032471\n",
      "Steps:725 , Loss: 0.9808630347251892\n",
      "Steps:730 , Loss: 0.609752893447876\n",
      "Epoch: 17 Step: 731 loss: 0.604014391816\n",
      "Steps:735 , Loss: 0.6126996278762817\n",
      "Steps:740 , Loss: 0.6202969551086426\n",
      "Steps:745 , Loss: 0.6460626125335693\n",
      "Steps:750 , Loss: 0.7679420709609985\n",
      "Steps:755 , Loss: 0.38624048233032227\n",
      "Steps:760 , Loss: 0.5710660815238953\n",
      "Steps:765 , Loss: 0.5355668067932129\n",
      "Steps:770 , Loss: 0.45529159903526306\n",
      "Epoch: 18 Step: 774 loss: 0.56606243932\n",
      "Train: F1 Score:  83.5623589355 Accuracy:  92.5145348837 Loss:  0.510108974091\n",
      "Val: F1 Score:  79.656483867 Accuracy:  90.15625 Loss:  0.588693866134\n",
      "Steps:775 , Loss: 0.4868113100528717\n",
      "Steps:780 , Loss: 0.5972278118133545\n",
      "Steps:785 , Loss: 0.47799739241600037\n",
      "Steps:790 , Loss: 0.6482481956481934\n",
      "Steps:795 , Loss: 0.8281021118164062\n",
      "Steps:800 , Loss: 0.5104836225509644\n",
      "Steps:805 , Loss: 0.5498068332672119\n",
      "Steps:810 , Loss: 0.4797297716140747\n",
      "Steps:815 , Loss: 0.6538016200065613\n",
      "Epoch: 19 Step: 817 loss: 0.570025791262\n",
      "Steps:820 , Loss: 0.5291327238082886\n",
      "Steps:825 , Loss: 0.5158247947692871\n",
      "Steps:830 , Loss: 0.485566109418869\n",
      "Steps:835 , Loss: 0.4280940592288971\n",
      "Steps:840 , Loss: 0.6050355434417725\n",
      "Steps:845 , Loss: 0.5526590347290039\n",
      "Steps:850 , Loss: 0.5455626249313354\n",
      "Steps:855 , Loss: 0.5966733694076538\n",
      "Steps:860 , Loss: 0.5290407538414001\n",
      "Epoch: 20 Step: 860 loss: 0.552974075772\n",
      "Train: F1 Score:  78.7519711373 Accuracy:  90.6613372093 Loss:  0.544241818578\n",
      "Val: F1 Score:  75.1828060202 Accuracy:  89.375 Loss:  0.633177950978\n",
      "Steps:865 , Loss: 0.38737326860427856\n",
      "Steps:870 , Loss: 0.4724639654159546\n",
      "Steps:875 , Loss: 0.49751362204551697\n",
      "Steps:880 , Loss: 0.7067157030105591\n",
      "Steps:885 , Loss: 0.47291168570518494\n",
      "Steps:890 , Loss: 0.5247032642364502\n",
      "Steps:895 , Loss: 0.3989759087562561\n",
      "Steps:900 , Loss: 0.5485316514968872\n",
      "Epoch: 21 Step: 903 loss: 0.546724101139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps:905 , Loss: 0.6080694198608398\n",
      "Steps:910 , Loss: 0.48801520466804504\n",
      "Steps:915 , Loss: 0.39554184675216675\n",
      "Steps:920 , Loss: 0.44944870471954346\n",
      "Steps:925 , Loss: 0.4250084459781647\n",
      "Steps:930 , Loss: 0.7480498552322388\n",
      "Steps:935 , Loss: 0.5143837928771973\n",
      "Steps:940 , Loss: 0.5468459129333496\n",
      "Steps:945 , Loss: 0.5343085527420044\n",
      "Epoch: 22 Step: 946 loss: 0.569423730983\n",
      "Train: F1 Score:  84.0833278948 Accuracy:  91.6424418605 Loss:  0.540815418543\n",
      "Val: F1 Score:  81.6788305042 Accuracy:  90.3125 Loss:  0.601367989182\n",
      "Steps:950 , Loss: 0.6205223798751831\n",
      "Steps:955 , Loss: 0.4968891143798828\n",
      "Steps:960 , Loss: 0.6018908023834229\n",
      "Steps:965 , Loss: 0.4586239159107208\n",
      "Steps:970 , Loss: 0.6373753547668457\n",
      "Steps:975 , Loss: 0.5906180739402771\n",
      "Steps:980 , Loss: 0.7135084271430969\n",
      "Steps:985 , Loss: 0.4305490553379059\n",
      "Epoch: 23 Step: 989 loss: 0.573463190434\n",
      "Steps:990 , Loss: 0.4952346086502075\n",
      "Steps:995 , Loss: 0.4546425938606262\n",
      "Steps:1000 , Loss: 0.5113011598587036\n",
      "Steps:1005 , Loss: 0.558582067489624\n",
      "Steps:1010 , Loss: 0.5670355558395386\n",
      "Steps:1015 , Loss: 0.39600738883018494\n",
      "Steps:1020 , Loss: 0.521121621131897\n",
      "Steps:1025 , Loss: 0.5357643365859985\n",
      "Steps:1030 , Loss: 0.5262836217880249\n",
      "Epoch: 24 Step: 1032 loss: 0.521787738384\n",
      "Train: F1 Score:  82.3094503247 Accuracy:  90.6613372093 Loss:  0.55500545474\n",
      "Val: F1 Score:  79.8581818813 Accuracy:  88.28125 Loss:  0.632327142358\n",
      "Steps:1035 , Loss: 0.696779727935791\n",
      "Steps:1040 , Loss: 0.6607831716537476\n",
      "Steps:1045 , Loss: 0.5076451897621155\n",
      "Steps:1050 , Loss: 0.5327945351600647\n",
      "Steps:1055 , Loss: 0.49050843715667725\n",
      "Steps:1060 , Loss: 0.4831053614616394\n",
      "Steps:1065 , Loss: 0.4785391390323639\n",
      "Steps:1070 , Loss: 0.49915921688079834\n",
      "Steps:1075 , Loss: 0.6261106729507446\n",
      "Epoch: 25 Step: 1075 loss: 0.541372753853\n",
      "Steps:1080 , Loss: 0.3758854269981384\n",
      "Steps:1085 , Loss: 0.48732173442840576\n",
      "Steps:1090 , Loss: 0.6079614758491516\n",
      "Steps:1095 , Loss: 0.4752870798110962\n",
      "Steps:1100 , Loss: 0.4050023555755615\n",
      "Steps:1105 , Loss: 0.558821439743042\n",
      "Steps:1110 , Loss: 0.5022436380386353\n",
      "Steps:1115 , Loss: 0.49543020129203796\n",
      "Epoch: 26 Step: 1118 loss: 0.539187203313\n",
      "Train: F1 Score:  81.6237713574 Accuracy:  91.7877906977 Loss:  0.520391984734\n",
      "Val: F1 Score:  74.1959896382 Accuracy:  88.75 Loss:  0.598865881562\n",
      "Steps:1120 , Loss: 0.46334174275398254\n",
      "Steps:1125 , Loss: 0.5043063163757324\n",
      "Steps:1130 , Loss: 0.5411109924316406\n",
      "Steps:1135 , Loss: 0.5029133558273315\n",
      "Steps:1140 , Loss: 0.5270063281059265\n",
      "Steps:1145 , Loss: 0.7259271144866943\n",
      "Steps:1150 , Loss: 0.5885069966316223\n",
      "Steps:1155 , Loss: 0.475496768951416\n",
      "Steps:1160 , Loss: 0.460361123085022\n",
      "Epoch: 27 Step: 1161 loss: 0.555682943311\n",
      "Steps:1165 , Loss: 0.4404408931732178\n",
      "Steps:1170 , Loss: 0.6082017421722412\n",
      "Steps:1175 , Loss: 0.655287504196167\n",
      "Steps:1180 , Loss: 0.45444929599761963\n",
      "Steps:1185 , Loss: 0.5832341909408569\n",
      "Steps:1190 , Loss: 0.4334896206855774\n",
      "Steps:1195 , Loss: 0.5297402143478394\n",
      "Steps:1200 , Loss: 0.6559450626373291\n",
      "Epoch: 28 Step: 1204 loss: 0.56141862897\n",
      "Train: F1 Score:  72.4483197019 Accuracy:  89.425872093 Loss:  0.592463824638\n",
      "Val: F1 Score:  70.8810462316 Accuracy:  88.125 Loss:  0.649966558814\n",
      "Steps:1205 , Loss: 0.5044710636138916\n",
      "Steps:1210 , Loss: 0.4724104702472687\n",
      "Steps:1215 , Loss: 0.8508828282356262\n",
      "Steps:1220 , Loss: 0.6143473386764526\n",
      "Steps:1225 , Loss: 0.5111384391784668\n",
      "Steps:1230 , Loss: 0.4448058307170868\n",
      "Steps:1235 , Loss: 0.6931408643722534\n",
      "Steps:1240 , Loss: 0.5078231692314148\n",
      "Steps:1245 , Loss: 0.3779522478580475\n",
      "Epoch: 29 Step: 1247 loss: 0.565954213226\n",
      "Steps:1250 , Loss: 0.38615986704826355\n",
      "Steps:1255 , Loss: 0.5623500347137451\n",
      "Steps:1260 , Loss: 0.5054024457931519\n",
      "Steps:1265 , Loss: 0.38913464546203613\n",
      "Steps:1270 , Loss: 0.38384944200515747\n",
      "Steps:1275 , Loss: 0.4866821765899658\n",
      "Steps:1280 , Loss: 0.6792930364608765\n",
      "Steps:1285 , Loss: 0.5460518598556519\n",
      "Steps:1290 , Loss: 0.8637837171554565\n",
      "Epoch: 30 Step: 1290 loss: 0.527160128189\n",
      "Train: F1 Score:  82.054831759 Accuracy:  91.7514534884 Loss:  0.537473700767\n",
      "Val: F1 Score:  76.0936693437 Accuracy:  89.0625 Loss:  0.605520895123\n",
      "Test: F1 Score:  74.4955706187 Accuracy:  89.4230769231 Loss:  0.61924428894\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30  #number of epochs\n",
    "y_train = np.array(y_train)\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    random = np.random.choice(len(y_train), size=(len(y_train)), replace=False)\n",
    "    x_id_train = x_id_train[random]\n",
    "    y_train = y_train[random]\n",
    "    mask_train = mask_train[random]\n",
    "            \n",
    "    losses, step = model(x_id_train, y_train, mask_train)\n",
    "    print(\"Epoch:\", i+1,\"Step:\", step, \"loss:\",losses)\n",
    "#     saver.save(sess, save_path=save_dir)                         \n",
    "#     print(\"Saved Model Complete\")\n",
    "    \n",
    "    if((i+1)%2==0):\n",
    "        train_loss, train_pred = model(x_id_train, y_train, mask_train, train_cond=False)\n",
    "        f1_, accu_ = eval_score(y_train, train_pred)\n",
    "        print(\"Train: F1 Score: \",  f1_, \"Accuracy: \", accu_, \"Loss: \", train_loss)\n",
    "        val_loss, val_pred = model(x_id_val, y_val, mask_val, train_cond=False)\n",
    "        f1_, accu_ = eval_score(y_val, val_pred)\n",
    "        print(\"Val: F1 Score: \",  f1_, \"Accuracy: \", accu_, \"Loss: \", val_loss)\n",
    "        \n",
    "test_loss, test_pred = model(x_id_test, y_test, mask_test, train_cond=False)\n",
    "f1_, accu_ = eval_score(y_test, test_pred)\n",
    "print(\"Test: F1 Score: \",  f1_, \"Accuracy: \", accu_, \"Loss: \", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
