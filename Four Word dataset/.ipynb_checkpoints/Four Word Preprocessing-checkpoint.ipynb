{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import argparse\n",
    "from numpy import random\n",
    "\n",
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk import word_tokenize\n",
    "    from nltk import pos_tag\n",
    "except:\n",
    "    print(\"NLTK NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dominant_only = 1\n",
    "\n",
    "# datafile = \"~/Data/one-million-sense-tagged-instances-wn30\"\n",
    "split = 0.4\n",
    "window = 1\n",
    "windowSize = 5\n",
    "add_start_end = 1\n",
    "dominant_only = 0\n",
    "dominant_per = 0.2\n",
    "remove_punctuation = 0\n",
    "remove_stopwords = 0\n",
    "POS_tags = 0\n",
    "POS_RED = 0\n",
    "senseval = True\n",
    "pos=\"noun\"\n",
    "\n",
    "Sent = open('Sentences.txt', 'r').read().split('\\n')\n",
    "Sens = open('Senses.txt', 'r').read().split('\\n')\n",
    "sentStr = \"\"\n",
    "sensStr = \"\"\n",
    "\n",
    "wordList = {\n",
    "        'hard': ['HARD1', 'HARD2', 'HARD3'],\n",
    "        'interest': ['interest1', 'interest2', 'interest3', 'interest4', 'interest5', 'interest6'],\n",
    "        'line': ['text', 'phone', 'product', 'formation', 'division', 'cord'],\n",
    "        'serve': ['SERVE2', 'SERVE6', 'SERVE10', 'SERVE12']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkword = \"interest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checklist = wordList[checkword]\n",
    "for i in range(0, len(Sens)):\n",
    "    if Sens[i] in checklist:\n",
    "        sentStr += Sent[i] + \"\\n\"\n",
    "        sensStr += Sens[i] + \"\\n\"\n",
    "\n",
    "sentFile = open('Preprocess_Files/' + checkword + '/sent', 'w')\n",
    "sensFile = open('Preprocess_Files/' + checkword + '/sense', 'w')\n",
    "\n",
    "sentFile.write(sentStr)\n",
    "sensFile.write(sensStr)\n",
    "\n",
    "sentFile.close()\n",
    "sensFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Senseval Dataset\n",
      "For Word: line\n",
      "Total number of examples: 8292\n",
      "Number of training examples: 19637\n"
     ]
    }
   ],
   "source": [
    "Sent = open('Preprocess_Files/' + checkword + '/sent', \"r\").read().split('\\n')\n",
    "del Sent[len(Sent) - 1] # last string is empty\n",
    "Sens = open('Preprocess_Files/' + checkword + '/sense', \"r\").read().split()\n",
    "\n",
    "assert(len(Sent) == len(Sens))\n",
    "print(\"Using Senseval Dataset\")\n",
    "print(\"For Word: \" + checkword)\n",
    "print(\"Total number of examples: \" + str(len(Sent)))\n",
    "\n",
    "# Delete minor senses if flag positive\n",
    "if dominant_only == 1:\n",
    "    SenSet = list(set(Sens))\n",
    "    SenCnt = []\n",
    "    for i in SenSet:\n",
    "        SenCnt.append(Sens.count(i))\n",
    "\n",
    "    maxCnt = max(SenCnt)\n",
    "    SenRemove = []\n",
    "    for i in range(0, len(SenSet)):\n",
    "        if SenCnt[i] < dominant_per * maxCnt:\n",
    "            print(\"IGNORING\", SenSet[i], SenCnt[i])\n",
    "            SenRemove.append(SenSet[i])\n",
    "\n",
    "    tmpSt = []\n",
    "    tmpSe = []\n",
    "    for i in range(0, len(Sent)):\n",
    "        if Sens[i] not in SenRemove:\n",
    "            tmpSt.append(Sent[i])\n",
    "            tmpSe.append(Sens[i])\n",
    "    Sent = tmpSt\n",
    "    Sens = tmpSe\n",
    "\n",
    "##Use POS Tags if flag positive\n",
    "if POS_tags == 1:\n",
    "    for i in range(0, len(Sent)):\n",
    "        try:\n",
    "            tmp = word_tokenize(Sent[i])\n",
    "            tmp_tagged = pos_tag(tmp)\n",
    "        except UnicodeDecodeError as u:\n",
    "            Sent[i] = re.sub(r'[^\\x00-\\x7F]+',' ', Sent[i])\n",
    "            tmp = word_tokenize(Sent[i])\n",
    "            tmp_tagged = pos_tag(tmp)\n",
    "        tmp = \"\"\n",
    "        for j in range(0, len(tmp_tagged)):\n",
    "            tmptag = tmp_tagged[j][1].lower()\n",
    "\n",
    "            if POS_RED == 1:\n",
    "                if tmptag in ['$', '\\'', '(', ')', ',', '.', '--', ':', ';']:\n",
    "                    tmptag = 'PUNCTUATION'\n",
    "                elif tmptag in ['DT', 'EX']:\n",
    "                    tmptag = 'DETERMINER'\n",
    "                elif tmptag in ['jj', 'jjr', 'jjs']:\n",
    "                    tmptag = 'ADJ'\n",
    "                elif tmptag in ['nn', 'nnp', 'nnps', 'nns']:\n",
    "                    tmptag = 'NOUN'\n",
    "                elif tmptag in ['prp', 'prp$']:\n",
    "                    tmptag = 'PRONOUN'\n",
    "                elif tmptag in ['rb', 'rbr','rbs']:\n",
    "                    tmptag = 'ADVERB'\n",
    "                elif tmptag in ['vb', 'vbd', 'vbg','vbn','vbp','vbz']:\n",
    "                    tmptag = 'VERB'\n",
    "                elif tmptag in ['wdt', 'wp', 'wp$', 'wrb']:\n",
    "                    tmptag = 'WH_WORD'\n",
    "\n",
    "            tmp += \" \" + tmp_tagged[j][0] + \"_\" + tmptag\n",
    "        Sent[i] = tmp\n",
    "\n",
    "##Randomly shuffle sentences\n",
    "tmp = list(zip(Sent, Sens))\n",
    "random.shuffle(tmp)\n",
    "Sent[:], Sens[:] = zip(*tmp)\n",
    "\n",
    "##Get set of senses\n",
    "lenst = len(Sent)\n",
    "SenSet = list(set(Sens))\n",
    "SenSet.sort()\n",
    "SenCnt = []\n",
    "\n",
    "##Get count of each sense\n",
    "for i in SenSet:\n",
    "    SenCnt.append(Sens.count(i))\n",
    "\n",
    "##Most dominant sense\n",
    "maxCnt = max(SenCnt)\n",
    "\n",
    "Sent_div = []\n",
    "for i in range(0,len(SenSet)):\n",
    "    Sent_div.append([])\n",
    "\n",
    "newline_regex = re.compile(r\"\\n[\\n]*\")\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "\n",
    "for i in range(0, lenst):\n",
    "    Sent[i] = Sent[i].strip()\n",
    "\n",
    "    ##remove stopwords if flag positive\n",
    "    if remove_stopwords == 1:\n",
    "        tmp = Sent[i].split()\n",
    "        filtered_words = [word for word in tmp if word not in stopwords.words('english')]\n",
    "        Sent[i] = ' '.join(filtered_words)\n",
    "\n",
    "    ##remove punctuations if flag positive\n",
    "    if remove_punctuation == 1:\n",
    "        Sent[i] = ''.join(ch for ch in Sent[i] if ch not in exclude)\n",
    "\n",
    "    ##extract window around the word if window positive\n",
    "    if window == 1:\n",
    "        left, _ ,right = Sent[i].lower().partition(checkword)\n",
    "        n = windowSize\n",
    "        left = left.split()[-n:]\n",
    "        right = right.split()[:n]\n",
    "        #print(right)\n",
    "        #try:\n",
    "        #    del right[0]\n",
    "        #except:\n",
    "        #    pass\n",
    "\n",
    "        tmp = []\n",
    "        if len(left) < windowSize and add_start_end == 1:\n",
    "            for j in range(0,windowSize-len(left)):\n",
    "                tmp.append('START')\n",
    "        tmp.extend(left)\n",
    "        left = tmp\n",
    "\n",
    "        if add_start_end == 1:\n",
    "            for j in range(len(right), windowSize):\n",
    "                right.append('END')\n",
    "\n",
    "        Sent[i] = ' '.join(left + [checkword] + right)\n",
    "\n",
    "    ##Formatting the sentences\n",
    "    Sent[i] = Sent[i].strip()\n",
    "    Sent[i] = Sent[i].replace(\" \", \"\\n\")\n",
    "    Sent[i] = newline_regex.sub(\"\\n\", Sent[i])\n",
    "    if \"\\n\\n\" in Sent[i]:\n",
    "        print(Sent[i])\n",
    "    ind = SenSet.index(Sens[i])\n",
    "    Sent_div[ind].append(Sent[i])\n",
    "\n",
    "##Split Training and test sentences\n",
    "SentTest = []\n",
    "SensTest = []\n",
    "Sent = []\n",
    "Sens = []\n",
    "for i in range(0, len(SenSet)):\n",
    "    numTotal = len(Sent_div[i])\n",
    "    numTest = int( split * numTotal)\n",
    "    numTrain = numTotal - numTest\n",
    "\n",
    "    SentTest.extend(Sent_div[i][ 0: numTest ])\n",
    "    for j in range(0, numTest ):\n",
    "        SensTest.append(SenSet[i])\n",
    "\n",
    "    Sent.extend(Sent_div[i][ numTest: numTotal ])\n",
    "    for j in range(0, numTrain ):\n",
    "        Sens.append(SenSet[i])\n",
    "\n",
    "##Equalize the number of Senses\n",
    "lenst = len(Sent)\n",
    "for i in range(0, lenst):\n",
    "    ind = SenSet.index(Sens[i])\n",
    "    num = int(1.0*maxCnt/SenCnt[ind])\n",
    "    for j in range(0, num):\n",
    "        Sent.append(Sent[i])\n",
    "        Sens.append(Sens[i])\n",
    "\n",
    "##Write training set sentences\n",
    "tmp = list(zip(Sent, Sens))\n",
    "random.shuffle(tmp)\n",
    "Sent[:], Sens[:] = zip(*tmp)\n",
    "\n",
    "print(\"Number of training examples: \" + str(len(Sent)))\n",
    "# fileSent = open(\"BLSTM/text_words.csv\", \"w\")\n",
    "# fileSens = open(\"BLSTM/summary_words.csv\", \"w\")\n",
    "# tmp = '\\n\\n'.join(Sent)\n",
    "# fileSent.write(tmp)\n",
    "# tmp = '\\n\\n'.join(Sens)\n",
    "# fileSens.write(tmp)\n",
    "\n",
    "# try:\n",
    "#     ##Write test set sentences\n",
    "#     tmp = list(zip(SentTest, SensTest))\n",
    "#     random.shuffle(tmp)\n",
    "#     SentTest[:], SensTest[:] = zip(*tmp)\n",
    "#     print(\"Number of testing examples: \" + str(len(SentTest)))\n",
    "\n",
    "#     fileSentTest = open(\"BLSTM/test_text_words.csv\", \"w\")\n",
    "#     fileSensTest = open(\"BLSTM/test_summary_words.csv\", \"w\")\n",
    "#     tmp = '\\n\\n'.join(SentTest)\n",
    "#     fileSentTest.write(tmp)\n",
    "#     tmp = '\\n\\n'.join(SensTest)\n",
    "#     fileSensTest.write(tmp)\n",
    "# except:\n",
    "#     print(\"Number of testing examples: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
