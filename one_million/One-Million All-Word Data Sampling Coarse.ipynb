{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "f = open(\"../../dataset/sense/dict_sense-keys\", 'rb')\n",
    "dict_sense_keys = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"../../dataset/sense/dict_word-sense\", 'rb')\n",
    "dict_word_sense = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('../Glove/word_embedding_glove', 'rb')\n",
    "word_embedding = pickle.load(f)\n",
    "f.close()\n",
    "word_embedding = word_embedding[: len(word_embedding)-1]\n",
    "\n",
    "f = open('../Glove/vocab_glove', 'rb')\n",
    "vocab = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "word2id = dict((w, i) for i,w in enumerate(vocab))\n",
    "id2word = dict((i, w) for i,w in enumerate(vocab))\n",
    "\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "\n",
    "with open('/data/aviraj/dataset/raw_preprocess_train','rb') as f:\n",
    "    data=pickle.load(f)\n",
    "\n",
    "with open('/data/aviraj/dataset/fulldata_vocab_sense','rb') as f:\n",
    "    vocab_lex=pickle.load(f)\n",
    "\n",
    "lex2id = dict((s, i) for i,s in enumerate(vocab_lex))\n",
    "id2lex = dict((i, s) for i,s in enumerate(vocab_lex))\n",
    "\n",
    "print(len(vocab_lex))\n",
    "max_sent_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "_pos = []\n",
    "for i in range(len(data)):\n",
    "    for pp in data[i][4]:\n",
    "        _pos.append(pp)\n",
    "        \n",
    "pos_count = Counter(_pos)\n",
    "pos_count = pos_count.most_common()\n",
    "vocab_pos = [pp for pp, c in pos_count]\n",
    "pos2id = dict((s, i) for i,s in enumerate(vocab_pos))\n",
    "print(len(vocab_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_y1 = []\n",
    "data_y2 = []\n",
    "data_y3 = []\n",
    "for i in range(len(data)):\n",
    "    if (len(data[i][1])<=200):\n",
    "        for j in range(len(data[i][2])):\n",
    "            if data[i][2][j] is not None:\n",
    "                data_y1.append(dict_sense_keys[data[i][2][j]][3])\n",
    "                data_y2.append(dict_sense_keys[data[i][2][j]][4])\n",
    "                data_y3.append(dict_sense_keys[data[i][2][j]][5])\n",
    "\n",
    "sense_count1 = Counter(data_y1)\n",
    "sense_count1 = sense_count1.most_common()\n",
    "sense_count2 = Counter(data_y2)\n",
    "sense_count4 = sense_count2.most_common(272)\n",
    "sense_count2 = sense_count2.most_common(312)\n",
    "sense_count3 = Counter(data_y3)\n",
    "sense_count5 = sense_count3.most_common(505)\n",
    "sense_count3 = sense_count3.most_common(1051)\n",
    "\n",
    "dict_sense_count1 = dict(sense_count1)\n",
    "dict_sense_count2 = dict(sense_count2)\n",
    "dict_sense_count3 = dict(sense_count3)\n",
    "dict_sense_count4 = dict(sense_count4)\n",
    "dict_sense_count5 = dict(sense_count5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 312 1051 272 505\n"
     ]
    }
   ],
   "source": [
    "print(len(sense_count1), len(sense_count2), len(sense_count3), len(sense_count4), len(sense_count5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_x = []\n",
    "data_pos = []\n",
    "data_label1 = []\n",
    "data_label2 = []\n",
    "data_label3 = []\n",
    "data_label4 = []\n",
    "data_label5 = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if not all(np.array(data[i][2])==None) and (len(data[i][1])<=200):\n",
    "        data_label1.append([ss if ss is not None and dict_sense_keys[ss][3] in dict_sense_count1 else None for ss in data[i][2]])\n",
    "        data_label2.append([ss if ss is not None and dict_sense_keys[ss][4] in dict_sense_count2 else None for ss in data[i][2]])\n",
    "        data_label3.append([ss if ss is not None and dict_sense_keys[ss][5] in dict_sense_count3 else None for ss in data[i][2]])\n",
    "        data_label4.append([ss if ss is not None and dict_sense_keys[ss][4] in dict_sense_count4 else None for ss in data[i][2]])\n",
    "        data_label5.append([ss if ss is not None and dict_sense_keys[ss][5] in dict_sense_count5 else None for ss in data[i][2]])\n",
    "        data_x.append(data[i][1])\n",
    "        data_pos.append(data[i][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_prepare(sense_id, x, pos, y, sense_count, lex_cond=False, pos_cond=False):\n",
    "    num_examples = len(x)\n",
    "    \n",
    "    vocab_sense = [s for s, c in sense_count]\n",
    "    sense2id = dict((s, i) for i,s in enumerate(vocab_sense))\n",
    "    \n",
    "    xx = np.zeros([num_examples, max_sent_size], dtype=int)\n",
    "    xx_mask = np.zeros([num_examples, max_sent_size], dtype=bool)\n",
    "    ss_mask = np.zeros([num_examples, max_sent_size], dtype=bool)\n",
    "    yy = np.zeros([num_examples,max_sent_size], dtype=int)\n",
    "    y_lex = np.zeros([num_examples, max_sent_size], dtype=int)\n",
    "    y_pos = np.zeros([num_examples, max_sent_size], dtype=int)\n",
    "        \n",
    "    for j in range(num_examples):\n",
    "        for i in range(max_sent_size):\n",
    "            if(i>=len(x[j])):\n",
    "                break\n",
    "            w = x[j][i]\n",
    "            s = y[j][i]\n",
    "            p = pos[j][i]\n",
    "            xx[j][i] = word2id[w] if w in word2id else word2id['UNKNOWN_TOKEN']\n",
    "            xx_mask[j][i] = True\n",
    "            ss_mask[j][i] = True if s is not None and dict_sense_keys[s][sense_id] in vocab_sense else False\n",
    "            yy[j][i] = sense2id[dict_sense_keys[s][sense_id]] if s is not None and dict_sense_keys[s][sense_id] in vocab_sense else 0\n",
    "            if(lex_cond):\n",
    "                y_lex[j][i] = lex2id[dict_sense_keys[s][3]] if s is not None and dict_sense_keys[s][3] in vocab_lex else len(vocab_lex)\n",
    "            if(pos_cond):\n",
    "                y_pos[j][i] = pos2id[p] if p in vocab_pos else len(vocab_pos)\n",
    "        \n",
    "    return xx, xx_mask, ss_mask, yy, y_lex, y_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_x = np.array(data_x)\n",
    "data_pos = np.array(data_pos)\n",
    "\n",
    "def train_val_data(name, sense_id, index, split_label, data_label, sense_count, sampling_list, lex_cond=False, pos_cond=False, sampling=False):\n",
    "    \n",
    "    index_train, index_val, label_train_id, label_val_id = train_test_split(index, split_label, train_size=0.8, shuffle=True, stratify=split_label, random_state=0)\n",
    "  \n",
    "    if(sampling):\n",
    "        dict_sample = dict(sampling_list)\n",
    "        sm = RandomOverSampler(ratio=dict_sample)\n",
    "        index_train1 = np.array(index_train).reshape(-1, 1)\n",
    "        sampled_index, _ = sm.fit_sample(index_train1, label_train_id)\n",
    "        count = Counter(_)\n",
    "        count = count.most_common()\n",
    "        sampled_index_train = np.array(sampled_index).reshape(1, -1)\n",
    "        index_train = sampled_index_train[0]\n",
    "    \n",
    "    data_label = np.array(data_label)\n",
    "    x_train = data_x[index_train]\n",
    "    y_train = data_label[index_train]\n",
    "    x_val = data_x[index_val]\n",
    "    y_val = data_label[index_val]\n",
    "    pos_train = []\n",
    "    pos_val = []\n",
    "    \n",
    "    if(pos_cond):\n",
    "        pos_train = data_pos[index_train]\n",
    "        pos_val = data_pos[index_val]\n",
    "\n",
    "    x_id_train, mask_train, sense_mask_train, y_id_train, lex_train, pos_id_train  = data_prepare(sense_id, x_train, pos_train, y_train, sense_count, lex_cond=lex_cond, pos_cond=pos_cond)\n",
    "    x_id_val, mask_val, sense_mask_val, y_id_val, lex_val, pos_id_val = data_prepare(sense_id, x_val, pos_val, y_val, sense_count, lex_cond=lex_cond, pos_cond=pos_cond)\n",
    "\n",
    "    train_data = {'x':x_id_train,'x_mask':mask_train, 'sense_mask':sense_mask_train, 'y':y_id_train, 'lex':lex_train, 'pos':pos_id_train}\n",
    "    val_data = {'x':x_id_val,'x_mask':mask_val, 'sense_mask':sense_mask_val, 'y':y_id_val, 'lex':lex_val, 'pos':pos_id_val}\n",
    "    \n",
    "    with open('/data/aviraj/dataset/train_val_data_coarse/all_word_'+ name,'wb') as f:\n",
    "        pickle.dump([train_data,val_data], f)\n",
    "    \n",
    "    print(len(x_id_train)+len(x_id_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850093\n",
      "850062\n",
      "850052\n",
      "849793\n",
      "848996\n"
     ]
    }
   ],
   "source": [
    "split_label1 = []\n",
    "split_label2 = []\n",
    "split_label3 = []\n",
    "split_label4 = []\n",
    "split_label5 = []\n",
    "\n",
    "index1 = []\n",
    "index2 = []\n",
    "index3 = []\n",
    "index4 = []\n",
    "index5 = []\n",
    "\n",
    "for jj, lab in enumerate(data_label1):\n",
    "    min_idx = np.argmin([dict_sense_count1[dict_sense_keys[lab[i]][3]] if lab[i] is not None else np.inf for i in range(len(lab))  ]) \n",
    "    if(lab[min_idx] is not None):\n",
    "        index1.append(jj)\n",
    "        split_label1.append(dict_sense_keys[lab[min_idx]][3])\n",
    "\n",
    "for jj, lab in enumerate(data_label2):\n",
    "    min_idx = np.argmin([dict_sense_count2[dict_sense_keys[lab[i]][4]] if lab[i] is not None else np.inf for i in range(len(lab))  ]) \n",
    "    if(lab[min_idx] is not None):\n",
    "        index2.append(jj)\n",
    "        split_label2.append(dict_sense_keys[lab[min_idx]][4])\n",
    "\n",
    "for jj, lab in enumerate(data_label3):\n",
    "    min_idx = np.argmin([dict_sense_count3[dict_sense_keys[lab[i]][5]] if lab[i] is not None else np.inf for i in range(len(lab))  ]) \n",
    "    if(lab[min_idx] is not None):\n",
    "        index3.append(jj)\n",
    "        split_label3.append(dict_sense_keys[lab[min_idx]][5])\n",
    "            \n",
    "for jj, lab in enumerate(data_label4):\n",
    "    min_idx = np.argmin([dict_sense_count4[dict_sense_keys[lab[i]][4]] if lab[i] is not None else np.inf for i in range(len(lab))  ]) \n",
    "    if(lab[min_idx] is not None):\n",
    "        index4.append(jj)\n",
    "        split_label4.append(dict_sense_keys[lab[min_idx]][4])\n",
    "\n",
    "for jj, lab in enumerate(data_label5):\n",
    "    min_idx = np.argmin([dict_sense_count5[dict_sense_keys[lab[i]][5]] if lab[i] is not None else np.inf for i in range(len(lab))  ]) \n",
    "    if(lab[min_idx] is not None):\n",
    "        index5.append(jj)\n",
    "        split_label5.append(dict_sense_keys[lab[min_idx]][5])\n",
    "            \n",
    "print(len(split_label1))\n",
    "print(len(split_label2))\n",
    "print(len(split_label3))\n",
    "print(len(split_label4))\n",
    "print(len(split_label5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/btech/aviraj/cs771/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850093\n",
      "850062\n",
      "850052\n",
      "849793\n",
      "848996\n",
      "848996\n"
     ]
    }
   ],
   "source": [
    "train_val_data('lex1', 3, index1, split_label1, data_label1, sense_count1, [], lex_cond=False, pos_cond=True)\n",
    "train_val_data('lex2', 3, index2, split_label2, data_label2, sense_count1, [], lex_cond=False, pos_cond=True)\n",
    "train_val_data('lex3', 3, index3, split_label3, data_label3, sense_count1, [], lex_cond=False, pos_cond=True)\n",
    "train_val_data('sense1', 4, index4, split_label4, data_label4, sense_count4, [], lex_cond=True, pos_cond=True)\n",
    "train_val_data('sense2', 4, index5, split_label5, data_label5, sense_count4, [], lex_cond=True, pos_cond=True)\n",
    "train_val_data('full_sense', 5, index5, split_label5, data_label5, sense_count5, [], lex_cond=True, pos_cond=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled_sense_count1 = [('1:19', 10000),\n",
    " ('1:17', 10000),\n",
    " ('2:34', 10000),\n",
    " ('2:33', 10000),\n",
    " ('1:27', 10000),\n",
    " ('2:37', 8000),\n",
    " ('1:24', 8000),\n",
    " ('1:08', 8000),\n",
    " ('1:12', 7000),\n",
    " ('1:22', 5000),\n",
    " ('2:29', 5000),\n",
    " ('1:05', 3000),\n",
    " ('1:16', 3000),\n",
    " ('1:25', 3000),\n",
    " ('1:20', 3000),\n",
    " ('1:13', 2000),\n",
    " ('2:43', 1100),\n",
    " ('3:44', 1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled_sense_count2= []\n",
    "for s, c in sense_count2[260:]:\n",
    "    sampled_sense_count2.append((s, 500))\n",
    "for s, c in sense_count2[180:260]:\n",
    "    sampled_sense_count2.append((s, 2000))\n",
    "for s, c in sense_count2[140:180]:\n",
    "    sampled_sense_count2.append((s, 5000))\n",
    "for s, c in sense_count2[75:140]:\n",
    "    sampled_sense_count2.append((s, 8000))\n",
    "for s, c in sense_count2[25:75]:\n",
    "    sampled_sense_count2.append((s, 12000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled_sense_count3= []\n",
    "for s, c in sense_count3[400:]:\n",
    "    sampled_sense_count3.append((s, 500))\n",
    "for s, c in sense_count3[200:400]:\n",
    "    sampled_sense_count3.append((s, 2000))\n",
    "for s, c in sense_count3[100:200]:\n",
    "    sampled_sense_count3.append((s, 5000))\n",
    "for s, c in sense_count3[70:100]:\n",
    "    sampled_sense_count3.append((s, 8000))\n",
    "for s, c in sense_count3[25:70]:\n",
    "    sampled_sense_count3.append((s, 12000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled_sense_count4= []\n",
    "for s, c in sense_count4[260:]:\n",
    "    sampled_sense_count4.append((s, 500))\n",
    "for s, c in sense_count4[180:260]:\n",
    "    sampled_sense_count4.append((s, 2000))\n",
    "for s, c in sense_count4[140:180]:\n",
    "    sampled_sense_count4.append((s, 5000))\n",
    "for s, c in sense_count4[75:140]:\n",
    "    sampled_sense_count4.append((s, 8000))\n",
    "for s, c in sense_count4[25:75]:\n",
    "    sampled_sense_count4.append((s, 12000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled_sense_count5= []\n",
    "for s, c in sense_count5[400:]:\n",
    "    sampled_sense_count5.append((s, 500))\n",
    "for s, c in sense_count5[200:400]:\n",
    "    sampled_sense_count5.append((s, 2000))\n",
    "for s, c in sense_count5[100:200]:\n",
    "    sampled_sense_count5.append((s, 5000))\n",
    "for s, c in sense_count5[70:100]:\n",
    "    sampled_sense_count5.append((s, 8000))\n",
    "for s, c in sense_count5[25:70]:\n",
    "    sampled_sense_count5.append((s, 12000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/btech/aviraj/cs771/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911174\n",
      "2061567\n",
      "2512876\n",
      "2041581\n",
      "2239996\n",
      "2239996\n"
     ]
    }
   ],
   "source": [
    "train_val_data('lex1_sampled', 3, index1, split_label1, data_label1, sense_count1, sampled_sense_count1, lex_cond=False, pos_cond=True, sampling=True)\n",
    "train_val_data('lex2_sampled', 3, index2, split_label2, data_label2, sense_count1, sampled_sense_count2, lex_cond=False, pos_cond=True, sampling=True)\n",
    "train_val_data('lex3_sampled', 3, index3, split_label3, data_label3, sense_count1, sampled_sense_count3, lex_cond=False, pos_cond=True, sampling=True)\n",
    "train_val_data('sense1_sampled', 4, index4, split_label4, data_label4, sense_count4, sampled_sense_count4, lex_cond=True, pos_cond=True, sampling=True)\n",
    "train_val_data('sense2_sampled', 4, index5, split_label5, data_label5, sense_count4, sampled_sense_count5, lex_cond=True, pos_cond=True, sampling=True)\n",
    "train_val_data('full_sense_sampled', 5, index5, split_label5, data_label5, sense_count5, sampled_sense_count5, lex_cond=True, pos_cond=True, sampling=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs771",
   "language": "python",
   "name": "cs771"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
