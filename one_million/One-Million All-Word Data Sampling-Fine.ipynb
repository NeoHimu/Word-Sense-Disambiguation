{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "f = open(\"../../dataset/sense/dict_sense-keys\", 'rb')\n",
    "dict_sense_keys = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"../../dataset/sense/dict_word-sense\", 'rb')\n",
    "dict_word_sense = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('../Glove/word_embedding_glove', 'rb')\n",
    "word_embedding = pickle.load(f)\n",
    "f.close()\n",
    "word_embedding = word_embedding[: len(word_embedding)-1]\n",
    "\n",
    "f = open('../Glove/vocab_glove', 'rb')\n",
    "vocab = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "word2id = dict((w, i) for i,w in enumerate(vocab))\n",
    "id2word = dict((i, w) for i,w in enumerate(vocab))\n",
    "\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "\n",
    "with open('../../dataset/raw_preprocess_train','rb') as f:\n",
    "    data=pickle.load(f)\n",
    "\n",
    "with open('../../dataset/fulldata_vocab_sense','rb') as f:\n",
    "    vocab_lex=pickle.load(f)\n",
    "\n",
    "lex2id = dict((s, i) for i,s in enumerate(vocab_lex))\n",
    "id2lex = dict((i, s) for i,s in enumerate(vocab_lex))\n",
    "\n",
    "print(len(vocab_lex))\n",
    "max_sent_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "_pos = []\n",
    "for i in range(len(data)):\n",
    "    for pp in data[i][4]:\n",
    "        _pos.append(pp)\n",
    "        \n",
    "pos_count = Counter(_pos)\n",
    "pos_count = pos_count.most_common()\n",
    "vocab_pos = [pp for pp, c in pos_count]\n",
    "pos2id = dict((s, i) for i,s in enumerate(vocab_pos))\n",
    "print(len(vocab_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y1 = []\n",
    "data_y2 = []\n",
    "data_y3 = []\n",
    "for i in range(len(data)):\n",
    "    if (len(data[i][1])<=200):\n",
    "        for j in range(len(data[i][2])):\n",
    "            if data[i][2][j] is not None:\n",
    "                data_y1.append(dict_sense_keys[data[i][2][j]][3])\n",
    "                data_y2.append(dict_sense_keys[data[i][2][j]][4])\n",
    "                data_y3.append(dict_sense_keys[data[i][2][j]][5])\n",
    "\n",
    "sense_count1 = Counter(data_y1)\n",
    "sense_count1 = sense_count1.most_common()[:-2]\n",
    "\n",
    "sense_count2 = Counter(data_y2)\n",
    "sense_count2 = sense_count2.most_common(180)\n",
    "\n",
    "sense_count3 = Counter(data_y3)\n",
    "sense_count3 = sense_count3.most_common(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sense1 = [s for s, c in sense_count1]\n",
    "vocab_sense2 = [s for s, c in sense_count2]\n",
    "vocab_sense3 = [s for s, c in sense_count3]\n",
    "\n",
    "vocab_sense1 = sorted(vocab_sense1, key=lambda x:int(x[0]))\n",
    "vocab_sense2 = sorted(vocab_sense2, key=lambda x:int(x[0]))\n",
    "vocab_sense3 = sorted(vocab_sense3, key=lambda x:int(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab_pos = ['NOUN', 'VERB', 'ADJ', 'ADV']\n",
    "for v in vocab_pos:\n",
    "    if v not in new_vocab_pos:\n",
    "        new_vocab_pos.append(v)\n",
    "vocab_pos = new_vocab_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mask_matrix(vocab_sense):\n",
    "    mask_mat = []\n",
    "    sense_list = [int(string[0]) if int(string[0]) !=5 else 3 for string in vocab_sense]\n",
    "    for i in range(len(set(sense_list))):\n",
    "        temp_row=[0]*len(sense_list)\n",
    "        for j in range(len(sense_list)):\n",
    "            if sense_list[j] == i+1:\n",
    "                temp_row[j]= 1\n",
    "            else:\n",
    "                temp_row[j]= 0\n",
    "        mask_mat.append(temp_row)\n",
    "    return mask_mat\n",
    "\n",
    "def hard_make_mask_matrix(vocab_sense):\n",
    "    mask_mat = []\n",
    "    sense_list = [int(string[0]) if int(string[0]) !=5 else 3 for string in vocab_sense]\n",
    "    for i in range(len(set(sense_list))):\n",
    "        temp_row=[0]*len(sense_list)\n",
    "        for j in range(len(sense_list)):\n",
    "            if sense_list[j] == i+1:\n",
    "                temp_row[j]= 0\n",
    "            else:\n",
    "                temp_row[j]= -10\n",
    "        mask_mat.append(temp_row)\n",
    "    return mask_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_mat1 = make_mask_matrix(vocab_sense1)\n",
    "mask_mat2 = make_mask_matrix(vocab_sense2)\n",
    "mask_mat3 = make_mask_matrix(vocab_sense3)\n",
    "\n",
    "with open('../../dataset/train_val_data_fine/mask_mat_lex','wb') as f:\n",
    "    pickle.dump(mask_mat1, f)\n",
    "with open('../../dataset/train_val_data_fine/mask_mat_sense','wb') as f:\n",
    "    pickle.dump(mask_mat2, f)\n",
    "with open('../../dataset/train_val_data_fine/mask_mat_full_sense','wb') as f:\n",
    "    pickle.dump(mask_mat3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_mask_mat1 = hard_make_mask_matrix(vocab_sense1)\n",
    "hard_mask_mat2 = hard_make_mask_matrix(vocab_sense2)\n",
    "hard_mask_mat3 = hard_make_mask_matrix(vocab_sense3)\n",
    "\n",
    "with open('../../dataset/train_val_data_fine/hard_mask_mat_lex','wb') as f:\n",
    "    pickle.dump(hard_mask_mat1, f)\n",
    "with open('../../dataset/train_val_data_fine/hard_mask_mat_sense','wb') as f:\n",
    "    pickle.dump(hard_mask_mat2, f)\n",
    "with open('../../dataset/train_val_data_fine/hard_mask_mat_full_sense','wb') as f:\n",
    "    pickle.dump(hard_mask_mat3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sense_count1 = dict(sense_count1)\n",
    "dict_sense_count2 = dict(sense_count2)\n",
    "dict_sense_count3 = dict(sense_count3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 180 300\n"
     ]
    }
   ],
   "source": [
    "print(len(sense_count1), len(sense_count2), len(sense_count3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = []\n",
    "data_pos = []\n",
    "data_label1 = []\n",
    "data_label2 = []\n",
    "data_label3 = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if not all(np.array(data[i][2])==None) and (len(data[i][1])<=200):\n",
    "        data_label1.append([ss if ss is not None and dict_sense_keys[ss][3] in dict_sense_count1 else None for ss in data[i][2]])\n",
    "        data_label2.append([ss if ss is not None and dict_sense_keys[ss][4] in dict_sense_count2 else None for ss in data[i][2]])\n",
    "        data_label3.append([ss if ss is not None and dict_sense_keys[ss][5] in dict_sense_count3 else None for ss in data[i][2]])\n",
    "        data_x.append(data[i][1])\n",
    "        data_pos.append(data[i][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare(sense_id, x, pos, y, sense_count, lex_cond=False, pos_cond=False):\n",
    "    num_examples = len(x)\n",
    "    \n",
    "    vocab_sense = [s for s, c in sense_count]\n",
    "    vocab_sense = sorted(vocab_sense, key=lambda x:int(x[0]))\n",
    "    sense2id = dict((s, i) for i,s in enumerate(vocab_sense))\n",
    "    \n",
    "    xx = np.zeros([num_examples, max_sent_size], dtype=int)\n",
    "    xx_mask = np.zeros([num_examples, max_sent_size], dtype=bool)\n",
    "    ss_mask = np.zeros([num_examples, max_sent_size], dtype=bool)\n",
    "    yy = np.zeros([num_examples,max_sent_size], dtype=int)\n",
    "    y_lex = np.zeros([num_examples, max_sent_size], dtype=int)\n",
    "    y_pos = np.zeros([num_examples, max_sent_size], dtype=int)\n",
    "    y_sense_pos = np.zeros([num_examples, max_sent_size], dtype=int)\n",
    "    \n",
    "    for j in range(num_examples):\n",
    "        for i in range(max_sent_size):\n",
    "            if(i>=len(x[j])):\n",
    "                break\n",
    "            w = x[j][i]\n",
    "            s = y[j][i]\n",
    "            p = pos[j][i]\n",
    "            xx[j][i] = word2id[w] if w in word2id else word2id['UNKNOWN_TOKEN']\n",
    "            xx_mask[j][i] = True\n",
    "            ss_mask[j][i] = True if s is not None and dict_sense_keys[s][sense_id] in vocab_sense else False\n",
    "            yy[j][i] = sense2id[dict_sense_keys[s][sense_id]] if s is not None and dict_sense_keys[s][sense_id] in vocab_sense else 0\n",
    "            y_sense_pos[j][i] = int(dict_sense_keys[s][1]) - 1 if s is not None else 0\n",
    "            if(lex_cond):\n",
    "                y_lex[j][i] = lex2id[dict_sense_keys[s][3]] if s is not None and dict_sense_keys[s][3] in vocab_lex else len(vocab_lex)\n",
    "            if(pos_cond):\n",
    "                y_pos[j][i] = pos2id[p] if p in vocab_pos else len(vocab_pos)\n",
    "        \n",
    "    return xx, xx_mask, ss_mask, yy, y_lex, y_pos, y_sense_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.array(data_x)\n",
    "data_pos = np.array(data_pos)\n",
    "\n",
    "def train_val_data(name, sense_id, index, split_label, data_label, sense_count, sampling_list, lex_cond=False, pos_cond=False, sampling=False):\n",
    "    \n",
    "    index_train, index_val, label_train_id, label_val_id = train_test_split(index, split_label, train_size=0.8, shuffle=True, stratify=split_label, random_state=0)\n",
    "  \n",
    "    if(sampling):\n",
    "        dict_sample = dict(sampling_list)\n",
    "        sm = RandomOverSampler(ratio=dict_sample)\n",
    "        index_train1 = np.array(index_train).reshape(-1, 1)\n",
    "        sampled_index, _ = sm.fit_sample(index_train1, label_train_id)\n",
    "        count = Counter(_)\n",
    "        count = count.most_common()\n",
    "        sampled_index_train = np.array(sampled_index).reshape(1, -1)\n",
    "        index_train = sampled_index_train[0]\n",
    "    \n",
    "    data_label = np.array(data_label)\n",
    "    x_train = data_x[index_train]\n",
    "    y_train = data_label[index_train]\n",
    "    x_val = data_x[index_val]\n",
    "    y_val = data_label[index_val]\n",
    "    pos_train = []\n",
    "    pos_val = []\n",
    "    \n",
    "    if(pos_cond):\n",
    "        pos_train = data_pos[index_train]\n",
    "        pos_val = data_pos[index_val]\n",
    "\n",
    "    x_id_train, mask_train, sense_mask_train, y_id_train, lex_train, pos_id_train, sense_pos_id_train  = data_prepare(sense_id, x_train, pos_train, y_train, sense_count, lex_cond=lex_cond, pos_cond=pos_cond)\n",
    "    x_id_val, mask_val, sense_mask_val, y_id_val, lex_val, pos_id_val, sense_pos_id_val = data_prepare(sense_id, x_val, pos_val, y_val, sense_count, lex_cond=lex_cond, pos_cond=pos_cond)\n",
    "\n",
    "    train_data = {'x':x_id_train,'x_mask':mask_train, 'sense_mask':sense_mask_train, 'y':y_id_train, 'sense_pos':sense_pos_id_train, 'lex':lex_train, 'pos':pos_id_train}\n",
    "    val_data = {'x':x_id_val,'x_mask':mask_val, 'sense_mask':sense_mask_val, 'y':y_id_val, 'sense_pos':sense_pos_id_val, 'lex':lex_val, 'pos':pos_id_val}\n",
    "    \n",
    "    with open('../../dataset/train_val_data_fine/all_word_'+ name,'wb') as f:\n",
    "        pickle.dump([train_data,val_data], f)\n",
    "    \n",
    "    print(len(x_id_train)+len(x_id_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850083\n",
      "838757\n",
      "828921\n"
     ]
    }
   ],
   "source": [
    "split_label1 = []\n",
    "split_label2 = []\n",
    "split_label3 = []\n",
    "\n",
    "index1 = []\n",
    "index2 = []\n",
    "index3 = []\n",
    "\n",
    "for jj, lab in enumerate(data_label1):\n",
    "    min_idx = np.argmin([dict_sense_count1[dict_sense_keys[lab[i]][3]] if lab[i] is not None else np.inf for i in range(len(lab))  ]) \n",
    "    if(lab[min_idx] is not None):\n",
    "        index1.append(jj)\n",
    "        split_label1.append(dict_sense_keys[lab[min_idx]][3])\n",
    "\n",
    "for jj, lab in enumerate(data_label2):\n",
    "    min_idx = np.argmin([dict_sense_count2[dict_sense_keys[lab[i]][4]] if lab[i] is not None else np.inf for i in range(len(lab))  ]) \n",
    "    if(lab[min_idx] is not None):\n",
    "        index2.append(jj)\n",
    "        split_label2.append(dict_sense_keys[lab[min_idx]][4])\n",
    "\n",
    "for jj, lab in enumerate(data_label3):\n",
    "    min_idx = np.argmin([dict_sense_count3[dict_sense_keys[lab[i]][5]] if lab[i] is not None else np.inf for i in range(len(lab))  ]) \n",
    "    if(lab[min_idx] is not None):\n",
    "        index3.append(jj)\n",
    "        split_label3.append(dict_sense_keys[lab[min_idx]][5])\n",
    "    \n",
    "print(len(split_label1))\n",
    "print(len(split_label2))\n",
    "print(len(split_label3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sshanukr/env/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850083\n",
      "838757\n",
      "828921\n"
     ]
    }
   ],
   "source": [
    "train_val_data('lex', 3, index1, split_label1, data_label1, sense_count1, [], lex_cond=False, pos_cond=True)\n",
    "train_val_data('sense', 4, index2, split_label2, data_label2, sense_count2, [], lex_cond=True, pos_cond=True)\n",
    "train_val_data('full_sense', 5, index3, split_label3, data_label3, sense_count3, [], lex_cond=True, pos_cond=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_sense_count1 = [('1:19', 10000),\n",
    " ('1:17', 10000),\n",
    " ('2:34', 10000),\n",
    " ('2:33', 10000),\n",
    " ('1:27', 10000),\n",
    " ('2:37', 8000),\n",
    " ('1:24', 8000),\n",
    " ('1:08', 8000),\n",
    " ('1:12', 7000),\n",
    " ('1:22', 5000),\n",
    " ('2:29', 5000),\n",
    " ('1:05', 3000),\n",
    " ('1:16', 3000),\n",
    " ('1:25', 3000),\n",
    " ('1:20', 3000),\n",
    " ('1:13', 2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_sense_count2= []\n",
    "for s, c in sense_count2[120:]:\n",
    "    sampled_sense_count2.append((s, 5000))\n",
    "for s, c in sense_count2[75:120]:\n",
    "    sampled_sense_count2.append((s, 8000))\n",
    "for s, c in sense_count2[25:75]:\n",
    "    sampled_sense_count2.append((s, 12000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_sense_count3= []\n",
    "for s, c in sense_count3[130:]:\n",
    "    sampled_sense_count3.append((s, 5000))\n",
    "for s, c in sense_count3[70:130]:\n",
    "    sampled_sense_count3.append((s, 8000))\n",
    "for s, c in sense_count3[25:70]:\n",
    "    sampled_sense_count3.append((s, 12000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sshanukr/env/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "909119\n",
      "1814988\n",
      "2375783\n"
     ]
    }
   ],
   "source": [
    "train_val_data('lex_sampled', 3, index1, split_label1, data_label1, sense_count1, sampled_sense_count1, lex_cond=False, pos_cond=True, sampling=True)\n",
    "train_val_data('sense_sampled', 4, index2, split_label2, data_label2, sense_count2, sampled_sense_count2, lex_cond=True, pos_cond=True, sampling=True)\n",
    "train_val_data('full_sense_sampled', 5, index3, split_label3, data_label3, sense_count3, sampled_sense_count3, lex_cond=True, pos_cond=True, sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
