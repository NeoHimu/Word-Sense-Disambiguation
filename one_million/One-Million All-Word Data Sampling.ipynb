{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "f = open(\"../../dataset/sense/dict_sense-keys\", 'rb')\n",
    "dict_sense_keys = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"../../dataset/sense/dict_word-sense\", 'rb')\n",
    "dict_word_sense = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('../Glove/word_embedding_glove', 'rb')\n",
    "word_embedding = pickle.load(f)\n",
    "f.close()\n",
    "word_embedding = word_embedding[: len(word_embedding)-1]\n",
    "\n",
    "f = open('../Glove/vocab_glove', 'rb')\n",
    "vocab = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "word2id = dict((w, i) for i,w in enumerate(vocab))\n",
    "id2word = dict((i, w) for i,w in enumerate(vocab))\n",
    "\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "\n",
    "with open('/data/aviraj/dataset/raw_preprocess_train','rb') as f:\n",
    "    data=pickle.load(f)\n",
    "\n",
    "with open('/data/aviraj/dataset/fulldata_vocab_sense','rb') as f:\n",
    "    vocab_lex=pickle.load(f)\n",
    "\n",
    "lex2id = dict((s, i) for i,s in enumerate(vocab_sense))\n",
    "id2lex = dict((i, s) for i,s in enumerate(vocab_sense))\n",
    "\n",
    "print(len(vocab_sense))\n",
    "\n",
    "num_senses = len(vocab_sense)\n",
    "max_sent_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pos = []\n",
    "for i in range(len(data)):\n",
    "    for pp in data[i][4]:\n",
    "        _pos.append(pp)\n",
    "        \n",
    "pos_count = Counter(_pos)\n",
    "pos_count = pos_count.most_common()\n",
    "vocab_pos = [pp for pp, c in pos_count]\n",
    "pos2id = dict((s, i) for i,s in enumerate(vocab_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y1 = []\n",
    "data_y2 = []\n",
    "data_y3 = []\n",
    "for i in range(len(data)):\n",
    "    if (len(data[i][1])<=200):\n",
    "        for j in range(len(data[i][2])):\n",
    "            if data[i][2][j] is not None:\n",
    "                data_y1.append(dict_sense_keys[data[i][2][j]][3])\n",
    "                data_y2.append(dict_sense_keys[data[i][2][j]][4])\n",
    "                data_y3.append(dict_sense_keys[data[i][2][j]][5])\n",
    "\n",
    "sense_count1 = Counter(data_y1)\n",
    "sense_count1 = sense_count1.most_common()\n",
    "sense_count2 = Counter(data_y2)\n",
    "sense_count2 = sense_count2.most_common(312)\n",
    "sense_count3 = Counter(data_y3)\n",
    "sense_count3 = sense_count3.most_common(1051)\n",
    "\n",
    "dict_sense_count1 = dict(sense_count1)\n",
    "dict_sense_count2 = dict(sense_count2)\n",
    "dict_sense_count3 = dict(sense_count3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 312 1051\n"
     ]
    }
   ],
   "source": [
    "print(len(sense_count1), len(sense_count2), len(sense_count3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = []\n",
    "data_pos = []\n",
    "data_label1 = []\n",
    "data_label2 = []\n",
    "data_label3 = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if not all(np.array(data[i][2])==None) and (len(data[i][1])<=200):\n",
    "        data_label1.append([ss if ss is not None and dict_sense_keys[ss][3] in dict_sense_count1 else None for ss in data[i][2]])\n",
    "        data_label2.append([ss if ss is not None and dict_sense_keys[ss][4] in dict_sense_count2 else None for ss in data[i][2]])\n",
    "        data_label3.append([ss if ss is not None and dict_sense_keys[ss][5] in dict_sense_count3 else None for ss in data[i][2]])\n",
    "        data_x.append(data[i][1])\n",
    "        data_pos.append(data[i][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850093\n",
      "850093\n",
      "850093\n"
     ]
    }
   ],
   "source": [
    "split_label1 = []\n",
    "split_label2 = []\n",
    "split_label3 = []\n",
    "index1 = []\n",
    "index2 = []\n",
    "index3 = []\n",
    "\n",
    "for jj, lab in enumerate(data_label1):\n",
    "        min_idx = np.argmin([dict_sense_count1[dict_sense_keys[lab[i]][3]] if lab[i] is not None else np.inf for i in range(len(lab))  ]) \n",
    "        if(min_idx != np.inf):\n",
    "            index1.append(jj)\n",
    "            split_label1.append(lab[min_idx])\n",
    "\n",
    "for jj, lab in enumerate(data_label2):\n",
    "        min_idx = np.argmin([dict_sense_count2[dict_sense_keys[lab[i]][4]] if lab[i] is not None else np.inf for i in range(len(lab))  ]) \n",
    "        if(min_idx != np.inf):\n",
    "            index2.append(jj)\n",
    "            split_label2.append(lab[min_idx])\n",
    "\n",
    "for jj, lab in enumerate(data_label3):\n",
    "        min_idx = np.argmin([dict_sense_count3[dict_sense_keys[lab[i]][5]] if lab[i] is not None else np.inf for i in range(len(lab))  ]) \n",
    "        if(min_idx != np.inf):\n",
    "            index3.append(jj)\n",
    "            split_label3.append(lab[min_idx])\n",
    "\n",
    "print(len(split_label1))\n",
    "print(len(split_label2))\n",
    "print(len(split_label3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_prepare(x, pos, y, sense_count, lex_cond=False, pos_cond=False):\n",
    "    num_examples = len(x)\n",
    "    \n",
    "    vocab_sense = [s for s, c in sense_count]\n",
    "    sense2id = dict((s[0], i) for i,s in enumerate(sense_count))\n",
    "    \n",
    "    xx = np.zeros([num_examples, max_sent_size], dtype=int)\n",
    "    xx_mask = np.zeros([num_examples, max_sent_size], dtype=bool)\n",
    "    ss_mask = np.zeros([num_examples, max_sent_size], dtype=bool)\n",
    "    yy = np.zeros([num_examples,max_sent_size], dtype=int)\n",
    "    y_lex = np.zeros([num_examples, max_sent_size], dtype=int)\n",
    "    y_pos = np.zeros([num_examples, max_sent_size], dtype=int)\n",
    "        \n",
    "    for j in range(num_examples):\n",
    "        for i in range(max_sent_size):\n",
    "            if(i>=len(x[j])):\n",
    "                break\n",
    "            w = x[j][i]\n",
    "            s = y[j][i]\n",
    "            p = pos[j][i]\n",
    "            xx[j][i] = word2id[w] if w in word2id else word2id['UNKNOWN_TOKEN']\n",
    "            xx_mask[j][i] = True\n",
    "            ss_mask[j][i] = True if s in vocab_sense else False\n",
    "            yy[j][i] = sense2id[s] if s in vocab_sense else 0\n",
    "            if(lex_cond):\n",
    "                y_lex[j][i] = lex2id[dict_sense_keys[s][3]] if in vocab_lex else len(vocab_lex)\n",
    "            if(pos_cond):\n",
    "                y_pos[j][i] = pos2id[p] if p in vocab_pos else len(vocab_pos)\n",
    "        \n",
    "    return xx, xx_mask, ss_mask, yy, y_lex, y_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_val_data(name, index, split_label, data_label, sense_count, lex_cond=False, pos_cond=False):\n",
    "    \n",
    "    index_train, index_val, label_train_id, label_val_id = train_test_split(index, split_label, train_size=0.8, shuffle=True, stratify=split_label, random_state=0)\n",
    "\n",
    "    data_x = np.array(data_x)\n",
    "    data_label = np.array(data_label)\n",
    "    \n",
    "    x_train = data_x[index_train]\n",
    "    y_train = data_label[index_train]\n",
    "    x_val = data_x[index_val]\n",
    "    y_val = data_label[index_val]\n",
    "    \n",
    "    pos_train = []\n",
    "    pos_val = []\n",
    "    \n",
    "    if(pos_cond):\n",
    "        pos_train = data_pos[index_train]\n",
    "        pos_val = data_pos[index_val]\n",
    "\n",
    "    x_id_train, mask_train, sense_mask_train, y_id_train, lex_train, pos_id_train  = data_prepare(x_train, pos_train, y_train, sense_count, lex_cond=lex_cond, pos_cond=pos_cond)\n",
    "    x_id_val, mask_val, sense_mask_val, y_id_val, lex_val, pos_id_val = data_prepare(x_val, pos_val, y_val, sense_count, lex_cond=lex_cond, pos_cond=pos_cond)\n",
    "\n",
    "    train_data = {'x':x_id_train,'x_mask':mask_train, 'sense_mask':sense_mask_train, 'y':y_id_train, 'lex':lex_train, 'pos':pos_id_train}\n",
    "    val_data = {'x':x_id_val,'x_mask':mask_val, 'sense_mask':sense_mask_val, 'y':y_id_val, 'lex':lex_val, 'pos':pos_id_val}\n",
    "    \n",
    "    with open('/data/aviraj/dataset/train_val_data/all_word_'+ name,'wb') as f:\n",
    "        pickle.dump([train_data,val_data], f)\n",
    "    \n",
    "    print(len(x_id_train)+len(x_id_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/btech/aviraj/cs771/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train_val_data(split_label1)\n",
    "train_val_data(split_label2)\n",
    "train_val_data(split_label3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sample = [('1:19', 10000),\n",
    " ('1:17', 10000),\n",
    " ('2:34', 10000),\n",
    " ('2:33', 10000),\n",
    " ('1:27', 10000),\n",
    " ('2:37', 8000),\n",
    " ('1:24', 8000),\n",
    " ('1:08', 8000),\n",
    " ('1:12', 7000),\n",
    " ('1:22', 5000),\n",
    " ('2:29', 5000),\n",
    " ('1:05', 3000),\n",
    " ('1:16', 3000),\n",
    " ('1:25', 3000),\n",
    " ('1:20', 3000),\n",
    " ('1:13', 2000),\n",
    " ('2:43', 1100),\n",
    " ('3:44', 1000)]\n",
    "\n",
    "dict_sample = dict(list_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741155 680074\n"
     ]
    }
   ],
   "source": [
    "sm = RandomOverSampler(ratio=dict_sample)\n",
    "\n",
    "index_train = np.array(index_train).reshape(-1, 1)\n",
    "sampled_index, _ = sm.fit_sample(index_train, label_train_id)\n",
    "\n",
    "print(len(sampled_index), len(index_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3:00', 59395),\n",
       " ('5:00', 56790),\n",
       " ('1:04', 52909),\n",
       " ('1:10', 40702),\n",
       " ('1:09', 38131),\n",
       " ('1:14', 33103),\n",
       " ('2:32', 30711),\n",
       " ('2:31', 28800),\n",
       " ('2:41', 25662),\n",
       " ('2:42', 25318),\n",
       " ('1:07', 25124),\n",
       " ('1:18', 21228),\n",
       " ('2:30', 19801),\n",
       " ('1:26', 19794),\n",
       " ('1:06', 19728),\n",
       " ('1:28', 19211),\n",
       " ('2:40', 15804),\n",
       " ('2:38', 12146),\n",
       " ('2:36', 12035),\n",
       " ('1:21', 11834),\n",
       " ('1:15', 11142),\n",
       " ('3:01', 10656),\n",
       " ('1:19', 10000),\n",
       " ('1:17', 10000),\n",
       " ('2:34', 10000),\n",
       " ('2:33', 10000),\n",
       " ('1:27', 10000),\n",
       " ('2:35', 9885),\n",
       " ('1:11', 9117),\n",
       " ('2:39', 8526),\n",
       " ('2:37', 8000),\n",
       " ('1:08', 8000),\n",
       " ('1:24', 8000),\n",
       " ('1:03', 7675),\n",
       " ('1:23', 7114),\n",
       " ('1:12', 7000),\n",
       " ('2:29', 5000),\n",
       " ('1:22', 5000),\n",
       " ('1:05', 3000),\n",
       " ('1:20', 3000),\n",
       " ('1:16', 3000),\n",
       " ('1:25', 3000),\n",
       " ('1:13', 2000),\n",
       " ('4:02', 1714),\n",
       " ('2:43', 1100),\n",
       " ('3:44', 1000)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = Counter(_)\n",
    "count = count.most_common()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741155 741155\n"
     ]
    }
   ],
   "source": [
    "sampled_index_train = np.array(sampled_index).reshape(1, -1)\n",
    "sampled_x_train = data_x[sampled_index_train[0]]\n",
    "sampled_y_train = data_label[sampled_index_train[0]]\n",
    "\n",
    "print(len(sampled_x_train), len(sampled_y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled_x_id_train, sampled_mask_train, sampled_sense_mask_train, sampled_y_train = data_prepare(sampled_x_train, sampled_y_train)\n",
    "\n",
    "with open('/data/aviraj/dataset/sampled_train_data','wb') as f:\n",
    "    pickle.dump([sampled_x_id_train, sampled_mask_train, sampled_sense_mask_train, sampled_y_train],f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs771",
   "language": "python",
   "name": "cs771"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
