{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "f = open(\"../../dataset/sense/dict_sense-keys\", 'rb')\n",
    "dict_sense_keys = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"../../dataset/sense/dict_word-sense\", 'rb')\n",
    "dict_word_sense = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('../Glove/word_embedding_glove', 'rb')\n",
    "word_embedding = pickle.load(f)\n",
    "f.close()\n",
    "word_embedding = word_embedding[: len(word_embedding)-1]\n",
    "\n",
    "f = open('../Glove/vocab_glove', 'rb')\n",
    "vocab = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "word2id = dict((w, i) for i,w in enumerate(vocab))\n",
    "id2word = dict((i, w) for i,w in enumerate(vocab))\n",
    "\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "\n",
    "with open('/data/aviraj/dataset/raw_preprocess_train','rb') as f:\n",
    "    data=pickle.load(f)\n",
    "\n",
    "with open('/data/aviraj/dataset/fulldata_vocab_sense','rb') as f:\n",
    "    vocab_sense=pickle.load(f)\n",
    "\n",
    "sense2id = dict((s, i) for i,s in enumerate(vocab_sense))\n",
    "id2sense = dict((i, s) for i,s in enumerate(vocab_sense))\n",
    "\n",
    "print(len(vocab_sense))\n",
    "\n",
    "num_senses = len(vocab_sense)\n",
    "max_sent_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_x = []\n",
    "data_label = []\n",
    "for i in range(len(data)):\n",
    "    if not all(np.array(data[i][2])==None) and (len(data[i][1])<=200):\n",
    "        data_label.append([dict_sense_keys[ss][3] if ss is not None else None for ss in data[i][2]])\n",
    "        data_x.append(data[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3:00', 105101),\n",
       " ('5:00', 99159),\n",
       " ('1:04', 85106),\n",
       " ('1:10', 66774),\n",
       " ('1:09', 61155),\n",
       " ('2:42', 55440),\n",
       " ('2:32', 52567),\n",
       " ('1:14', 51455),\n",
       " ('2:31', 45470),\n",
       " ('2:41', 40405),\n",
       " ('1:07', 39163),\n",
       " ('1:06', 38029),\n",
       " ('1:18', 37116),\n",
       " ('2:30', 32092),\n",
       " ('1:28', 29719),\n",
       " ('1:26', 29119),\n",
       " ('2:40', 24892),\n",
       " ('2:38', 20997),\n",
       " ('4:02', 18940),\n",
       " ('1:15', 18260),\n",
       " ('2:36', 17601),\n",
       " ('1:03', 17586),\n",
       " ('1:21', 16692),\n",
       " ('3:01', 15778),\n",
       " ('2:35', 15561),\n",
       " ('1:11', 12982),\n",
       " ('2:39', 12803),\n",
       " ('1:23', 10659),\n",
       " ('1:19', 6918),\n",
       " ('1:17', 6472),\n",
       " ('2:34', 6390),\n",
       " ('2:33', 6213),\n",
       " ('1:27', 6121),\n",
       " ('2:37', 6076),\n",
       " ('1:24', 6013),\n",
       " ('1:08', 5675),\n",
       " ('1:12', 4667),\n",
       " ('1:22', 3205),\n",
       " ('2:29', 3092),\n",
       " ('1:05', 1754),\n",
       " ('1:16', 1013),\n",
       " ('1:25', 938),\n",
       " ('1:20', 928),\n",
       " ('1:13', 812),\n",
       " ('2:43', 110),\n",
       " ('3:44', 35)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_y = []\n",
    "for i in range(len(data)):\n",
    "    if (len(data[i][1])<=200):\n",
    "        for j in range(len(data[i][2])):\n",
    "            if data[i][2][j] is not None:\n",
    "                data_y.append(dict_sense_keys[data[i][2][j]][3])\n",
    "\n",
    "sense_count = Counter(data_y)\n",
    "sense_count = sense_count.most_common()\n",
    "sense_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sense_count = dict(sense_count)\n",
    "\n",
    "split_label = []\n",
    "for lab in data_label:\n",
    "        min_idx = np.argmin([dict_sense_count[lab[i]] if lab[i] is not None else np.inf for i in range(len(lab))  ]) \n",
    "        split_label.append(lab[min_idx])\n",
    "\n",
    "print(len(split_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_prepare(x, y):\n",
    "    num_examples = len(x)\n",
    "\n",
    "    xx = np.zeros([num_examples, max_sent_size], dtype=int)\n",
    "    xx_mask = np.zeros([num_examples, max_sent_size], dtype=bool)\n",
    "    ss_mask = np.zeros([num_examples, max_sent_size], dtype=bool)\n",
    "    yy = np.zeros([num_examples,max_sent_size], dtype=int)\n",
    "    \n",
    "    for j in range(num_examples):\n",
    "        for i in range(max_sent_size):\n",
    "            if(i>=len(x[j])):\n",
    "                break\n",
    "            w = x[j][i]\n",
    "            s = y[j][i]\n",
    "            xx[j][i] = word2id[w] if w in word2id else word2id['UNKNOWN_TOKEN']\n",
    "            xx_mask[j][i] = True\n",
    "            ss_mask[j][i] = True if s in vocab_sense else False\n",
    "            yy[j][i] = sense2id[s] if s in vocab_sense else 0\n",
    "        \n",
    "    return xx, xx_mask, ss_mask, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "for i in range(len(data_x)):\n",
    "    index.append(i)\n",
    "\n",
    "index_train, index_val, label_train_id, label_val_id = train_test_split(index, split_label, train_size=0.8, shuffle=True, stratify=split_label, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_x = np.array(data_x)\n",
    "data_label = np.array(data_label)\n",
    "\n",
    "x_train = data_x[index_train]\n",
    "y_train = data_label[index_train]\n",
    "x_val = data_x[index_val]\n",
    "y_val = data_label[index_val]\n",
    "\n",
    "x_id_train, mask_train, sense_mask_train, y_train = data_prepare(x_train, y_train)\n",
    "x_id_val, mask_val, sense_mask_val, y_val = data_prepare(x_val, y_val)\n",
    "\n",
    "with open('/data/aviraj/dataset/train_data','wb') as f:\n",
    "    pickle.dump([x_id_train,mask_train,sense_mask_train,y_train],f)\n",
    "    \n",
    "with open('/data/aviraj/dataset/val_data','wb') as f:\n",
    "    pickle.dump([x_id_val,mask_val,sense_mask_val,y_val],f)\n",
    "    \n",
    "print(len(x_id_train)+len(x_id_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sample = [('1:19', 10000),\n",
    " ('1:17', 10000),\n",
    " ('2:34', 10000),\n",
    " ('2:33', 10000),\n",
    " ('1:27', 10000),\n",
    " ('2:37', 8000),\n",
    " ('1:24', 8000),\n",
    " ('1:08', 8000),\n",
    " ('1:12', 7000),\n",
    " ('1:22', 5000),\n",
    " ('2:29', 5000),\n",
    " ('1:05', 3000),\n",
    " ('1:16', 3000),\n",
    " ('1:25', 3000),\n",
    " ('1:20', 3000),\n",
    " ('1:13', 2000),\n",
    " ('2:43', 1100),\n",
    " ('3:44', 1000)]\n",
    "\n",
    "dict_sample = dict(list_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm = RandomOverSampler(ratio=dict_sample)\n",
    "\n",
    "index_train = np.array(index_train).reshape(-1, 1)\n",
    "sampled_index, _ = sm.fit_sample(index_train, label_train_id)\n",
    "\n",
    "print(len(sampled_index_train[0]), len(index_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = Counter(_)\n",
    "count = count.most_common()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sampled_index_train = np.array(sampled_index).reshape(1, -1)\n",
    "sampled_x_train = data_x[sampled_index_train[0]]\n",
    "sampled_y_train = data_label[sampled_index_train[0]]\n",
    "\n",
    "print(len(sampled_x_train), len(sampled_y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_x_id_train, sampled_mask_train, sampled_sense_mask_train, sampled_y_train = data_prepare(sampled_x_train, sampled_y_train)\n",
    "\n",
    "with open('/data/aviraj/dataset/sampled_train_data','wb') as f:\n",
    "    pickle.dump([sampled_x_id_train, sampled_mask_train, sampled_sense_mask_train, sampled_y_train],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs771",
   "language": "python",
   "name": "cs771"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
