{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/data/aviraj/dataset/raw_preprocess_test','rb') as f:\n",
    "    global_data=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/data/aviraj/dataset/ALL.gold.key.txt','r') as f:\n",
    "    data_key=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'art',\n",
       " 'of',\n",
       " 'change_ringing',\n",
       " 'be',\n",
       " 'peculiar',\n",
       " 'to',\n",
       " 'the',\n",
       " 'english',\n",
       " ',',\n",
       " 'and',\n",
       " ',',\n",
       " 'like',\n",
       " 'most',\n",
       " 'english',\n",
       " 'peculiarity',\n",
       " ',',\n",
       " 'unintelligible',\n",
       " 'to',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_data[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_word_data(checkword):\n",
    "    \n",
    "    dataset_line=[]\n",
    "    for i,list_ in enumerate(global_data): \n",
    "        ind=[idx for idx,it in enumerate(list_[3]) if it==checkword]\n",
    "        for ii in ind:\n",
    "            if list_[2][ii] is not None:\n",
    "                dataset_line.append([list_[2][ii],list_[1],list_[4]])\n",
    "    \n",
    "    print(len(dataset_line))\n",
    "    with open('/data/aviraj/dataset/checkwords/'+checkword + '_data_test', 'wb') as f:\n",
    "        pickle.dump(dataset_line, f)\n",
    "    with open('/data/aviraj/dataset/checkwords/'+checkword + '_data_test', 'rb') as f:\n",
    "        data_ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_words = ['force', 'make', 'open', 'place', 'point', 'serve', 'support']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "31\n",
      "4\n",
      "5\n",
      "11\n",
      "2\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "for word in test_words:\n",
    "    make_word_data(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Glove/vocab_glove', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_words = []\n",
    "for sent in global_data:\n",
    "    train_words.extend(sent[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_words), len(set(train_words)), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "unknown_words = []\n",
    "for word in set(train_words):\n",
    "    if word not in vocab:\n",
    "        unknown_words.append(word)\n",
    "        \n",
    "un_counter = collections.Counter(unknown_words)\n",
    "un_counter = dict(un_counter)\n",
    "\n",
    "sorted_un_counter = sorted(un_counter.items(), key=lambda x:x[1], reverse=True)\n",
    "sorted_un_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('million_unknown_words.pickle', 'wb') as f:\n",
    "    pickle.dump(unknown_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sorted(global_data, key=lambda x:len(x[1]), reverse=True)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envs",
   "language": "python",
   "name": "cs771"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
